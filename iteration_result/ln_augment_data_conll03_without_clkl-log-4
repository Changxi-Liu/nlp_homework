/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2022-04-03 08:01:41,013 Reading data from /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4
2022-04-03 08:01:41,013 Train: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/train.txt
2022-04-03 08:01:41,013 Dev: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/dev.txt
2022-04-03 08:01:41,013 Test: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/test.txt
2022-04-03 08:02:23,989 {b'<unk>': 0, b'O': 1, b'S-ORG': 2, b'S-MISC': 3, b'S-X': 4, b'B-PER': 5, b'E-PER': 6, b'S-LOC': 7, b'B-ORG': 8, b'E-ORG': 9, b'I-PER': 10, b'S-PER': 11, b'B-MISC': 12, b'I-MISC': 13, b'E-MISC': 14, b'I-ORG': 15, b'B-LOC': 16, b'E-LOC': 17, b'I-LOC': 18, b'<START>': 19, b'<STOP>': 20}
2022-04-03 08:02:23,989 Corpus: 14987 train + 3466 dev + 3684 test sentences
/home/miao/6207/lcx/CLNER/flair/utils/params.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  dict_merge.dict_merge(params_dict, yaml.load(f))
[2022-04-03 08:02:25,008 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-03 08:02:25,009 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-03 08:02:26,074 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
[2022-04-03 08:02:27,695 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-03 08:02:27,696 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-03 08:02:27,741 INFO] loading weights file https://cdn.huggingface.co/xlm-roberta-large-pytorch_model.bin from cache at /home/miao/.cache/torch/transformers/a89d1c4637c1ea5ecd460c2a7c06a03acc9a961fc8c59aa2dd76d8a7f1e94536.2f41fe28a80f2730715b795242a01fc3dda846a85e7903adb3907dc5c5a498bf
[2022-04-03 08:02:43,446 INFO] All model checkpoint weights were used when initializing XLMRobertaModel.

[2022-04-03 08:02:43,446 INFO] All the weights of XLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use XLMRobertaModel for predictions without further training.
2022-04-03 08:02:47,252 Model Size: 559912398
Corpus: 14987 train + 3466 dev + 3684 test sentences
2022-04-03 08:02:47,303 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,305 Model: "FastSequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): TransformerWordEmbeddings(
      (model): XLMRobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(250002, 1024, padding_idx=1)
          (position_embeddings): Embedding(514, 1024, padding_idx=1)
          (token_type_embeddings): Embedding(1, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (12): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (13): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (14): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (15): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (16): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (17): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (18): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (19): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (20): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (21): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (22): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (23): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.1)
  (linear): Linear(in_features=1024, out_features=21, bias=True)
)"
2022-04-03 08:02:47,305 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,305 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2022-04-03 08:02:47,305 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,305 Parameters:
2022-04-03 08:02:47,305  - Optimizer: "AdamW"
2022-04-03 08:02:47,305  - learning_rate: "5e-06"
2022-04-03 08:02:47,305  - mini_batch_size: "4"
2022-04-03 08:02:47,305  - patience: "10"
2022-04-03 08:02:47,305  - anneal_factor: "0.5"
2022-04-03 08:02:47,305  - max_epochs: "5"
2022-04-03 08:02:47,306  - shuffle: "True"
2022-04-03 08:02:47,306  - train_with_dev: "False"
2022-04-03 08:02:47,306  - word min_freq: "-1"
2022-04-03 08:02:47,306 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,306 Model training base path: "resources/taggers/ln_augment_data_conll03_without_clkl4"
2022-04-03 08:02:47,306 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,306 Device: cuda:0
2022-04-03 08:02:47,306 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:47,306 Embeddings storage mode: none
2022-04-03 08:02:51,339 ----------------------------------------------------------------------------------------------------
2022-04-03 08:02:51,343 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-03 08:02:52,369 epoch 1 - iter 0/3747 - loss 886.68469238 - samples/sec: 3.90 - decode_sents/sec: 93.41
2022-04-03 08:04:45,580 epoch 1 - iter 374/3747 - loss 156.41059127 - samples/sec: 13.87 - decode_sents/sec: 79980.10
2022-04-03 08:06:37,236 epoch 1 - iter 748/3747 - loss 88.31766514 - samples/sec: 14.10 - decode_sents/sec: 46242.85
2022-04-03 08:08:28,628 epoch 1 - iter 1122/3747 - loss 63.31517082 - samples/sec: 14.13 - decode_sents/sec: 61687.61
2022-04-03 08:10:22,160 epoch 1 - iter 1496/3747 - loss 50.26115842 - samples/sec: 13.84 - decode_sents/sec: 48022.22
2022-04-03 08:12:23,507 epoch 1 - iter 1870/3747 - loss 42.21726391 - samples/sec: 12.93 - decode_sents/sec: 38876.81
2022-04-03 08:14:16,023 epoch 1 - iter 2244/3747 - loss 36.79069628 - samples/sec: 14.01 - decode_sents/sec: 46451.92
2022-04-03 08:16:04,192 epoch 1 - iter 2618/3747 - loss 32.81620614 - samples/sec: 14.58 - decode_sents/sec: 38467.82
2022-04-03 08:17:55,852 epoch 1 - iter 2992/3747 - loss 29.80505173 - samples/sec: 14.10 - decode_sents/sec: 153407.63
2022-04-03 08:19:49,366 epoch 1 - iter 3366/3747 - loss 27.42994103 - samples/sec: 13.85 - decode_sents/sec: 68306.23
2022-04-03 08:21:41,026 epoch 1 - iter 3740/3747 - loss 25.51882111 - samples/sec: 14.13 - decode_sents/sec: 39270.49
2022-04-03 08:21:42,475 ----------------------------------------------------------------------------------------------------
2022-04-03 08:21:42,475 EPOCH 1 done: loss 6.3722 - lr 0.05
2022-04-03 08:21:42,475 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-03 08:23:26,870 Macro Average: 95.00	Macro avg loss: 0.50
ColumnCorpus-CONLL03FULL	95.00	
2022-04-03 08:23:27,121 ----------------------------------------------------------------------------------------------------
2022-04-03 08:23:27,121 BAD EPOCHS (no improvement): 11
2022-04-03 08:23:27,121 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-03 08:23:27,121 ==================Saving the current best model: 95.0==================
2022-04-03 08:23:30,660 ----------------------------------------------------------------------------------------------------
2022-04-03 08:23:30,666 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-03 08:23:30,942 epoch 2 - iter 0/3747 - loss 7.18229675 - samples/sec: 14.53 - decode_sents/sec: 141.56
2022-04-03 08:25:21,985 epoch 2 - iter 374/3747 - loss 7.40018445 - samples/sec: 14.22 - decode_sents/sec: 43078.75
2022-04-03 08:27:11,597 epoch 2 - iter 748/3747 - loss 7.41043179 - samples/sec: 14.39 - decode_sents/sec: 65150.85
2022-04-03 08:28:57,891 epoch 2 - iter 1122/3747 - loss 7.34440587 - samples/sec: 14.84 - decode_sents/sec: 175633.40
2022-04-03 08:30:50,168 epoch 2 - iter 1496/3747 - loss 7.29274908 - samples/sec: 13.99 - decode_sents/sec: 41492.04
2022-04-03 08:32:46,121 epoch 2 - iter 1870/3747 - loss 7.27097902 - samples/sec: 13.56 - decode_sents/sec: 90327.34
2022-04-03 08:34:41,419 epoch 2 - iter 2244/3747 - loss 7.21303188 - samples/sec: 13.66 - decode_sents/sec: 46664.72
2022-04-03 08:36:43,977 epoch 2 - iter 2618/3747 - loss 7.14841152 - samples/sec: 12.81 - decode_sents/sec: 144441.40
2022-04-03 08:38:36,315 epoch 2 - iter 2992/3747 - loss 7.09230147 - samples/sec: 14.03 - decode_sents/sec: 44001.96
2022-04-03 08:40:27,129 epoch 2 - iter 3366/3747 - loss 7.00126750 - samples/sec: 14.25 - decode_sents/sec: 97102.69
2022-04-03 08:42:20,186 epoch 2 - iter 3740/3747 - loss 6.98959898 - samples/sec: 13.95 - decode_sents/sec: 64380.11
2022-04-03 08:42:22,044 ----------------------------------------------------------------------------------------------------
2022-04-03 08:42:22,044 EPOCH 2 done: loss 1.7483 - lr 0.04000000000000001
2022-04-03 08:42:22,045 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-03 08:44:10,050 Macro Average: 95.40	Macro avg loss: 0.49
ColumnCorpus-CONLL03FULL	95.40	
2022-04-03 08:44:10,209 ----------------------------------------------------------------------------------------------------
2022-04-03 08:44:10,210 BAD EPOCHS (no improvement): 11
2022-04-03 08:44:10,210 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-03 08:44:10,210 ==================Saving the current best model: 95.39999999999999==================
2022-04-03 08:44:19,188 ----------------------------------------------------------------------------------------------------
2022-04-03 08:44:19,197 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-03 08:44:19,286 epoch 3 - iter 0/3747 - loss 0.00551987 - samples/sec: 44.93 - decode_sents/sec: 361.53
2022-04-03 08:46:18,328 epoch 3 - iter 374/3747 - loss 6.21998351 - samples/sec: 13.19 - decode_sents/sec: 127814.69
2022-04-03 08:48:08,937 epoch 3 - iter 748/3747 - loss 6.20884362 - samples/sec: 14.26 - decode_sents/sec: 40333.48
2022-04-03 08:49:57,959 epoch 3 - iter 1122/3747 - loss 6.11502330 - samples/sec: 14.46 - decode_sents/sec: 31023.76
2022-04-03 08:51:51,037 epoch 3 - iter 1496/3747 - loss 6.07783133 - samples/sec: 13.93 - decode_sents/sec: 40067.17
2022-04-03 08:53:41,030 epoch 3 - iter 1870/3747 - loss 6.06613122 - samples/sec: 14.32 - decode_sents/sec: 54180.61
2022-04-03 08:55:37,038 epoch 3 - iter 2244/3747 - loss 6.07620248 - samples/sec: 13.57 - decode_sents/sec: 41932.39
2022-04-03 08:57:30,334 epoch 3 - iter 2618/3747 - loss 6.06259359 - samples/sec: 13.90 - decode_sents/sec: 30856.55
2022-04-03 08:59:28,606 epoch 3 - iter 2992/3747 - loss 6.05312153 - samples/sec: 13.30 - decode_sents/sec: 42817.42
2022-04-03 09:01:20,043 epoch 3 - iter 3366/3747 - loss 6.06290631 - samples/sec: 14.17 - decode_sents/sec: 51556.46
2022-04-03 09:03:13,417 epoch 3 - iter 3740/3747 - loss 6.06055134 - samples/sec: 13.91 - decode_sents/sec: 318172.44
2022-04-03 09:03:15,263 ----------------------------------------------------------------------------------------------------
2022-04-03 09:03:15,263 EPOCH 3 done: loss 1.5158 - lr 0.03
2022-04-03 09:03:15,263 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-03 09:04:54,314 Macro Average: 95.79	Macro avg loss: 0.47
ColumnCorpus-CONLL03FULL	95.79	
2022-04-03 09:04:54,556 ----------------------------------------------------------------------------------------------------
2022-04-03 09:04:54,556 BAD EPOCHS (no improvement): 11
2022-04-03 09:04:54,556 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-03 09:04:54,556 ==================Saving the current best model: 95.78999999999999==================
2022-04-03 09:05:03,441 ----------------------------------------------------------------------------------------------------
2022-04-03 09:05:03,456 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-03 09:05:03,809 epoch 4 - iter 0/3747 - loss 5.30775452 - samples/sec: 11.35 - decode_sents/sec: 130.03
2022-04-03 09:06:55,782 epoch 4 - iter 374/3747 - loss 5.60945235 - samples/sec: 14.07 - decode_sents/sec: 76055.79
2022-04-03 09:08:57,151 epoch 4 - iter 748/3747 - loss 5.71459657 - samples/sec: 12.93 - decode_sents/sec: 43140.65
2022-04-03 09:10:47,646 epoch 4 - iter 1122/3747 - loss 5.61419014 - samples/sec: 14.27 - decode_sents/sec: 50589.19
2022-04-03 09:12:38,450 epoch 4 - iter 1496/3747 - loss 5.59095501 - samples/sec: 14.24 - decode_sents/sec: 47206.43
2022-04-03 09:14:30,738 epoch 4 - iter 1870/3747 - loss 5.59693776 - samples/sec: 14.02 - decode_sents/sec: 30489.80
2022-04-03 09:16:23,477 epoch 4 - iter 2244/3747 - loss 5.55425791 - samples/sec: 13.97 - decode_sents/sec: 36844.21
2022-04-03 09:18:10,276 epoch 4 - iter 2618/3747 - loss 5.54384783 - samples/sec: 14.73 - decode_sents/sec: 36599.85
2022-04-03 09:20:03,400 epoch 4 - iter 2992/3747 - loss 5.52092845 - samples/sec: 13.91 - decode_sents/sec: 43696.14
2022-04-03 09:21:53,144 epoch 4 - iter 3366/3747 - loss 5.52520444 - samples/sec: 14.37 - decode_sents/sec: 32597.43
2022-04-03 09:23:58,935 epoch 4 - iter 3740/3747 - loss 5.52045606 - samples/sec: 12.44 - decode_sents/sec: 219424.17
2022-04-03 09:24:00,408 ----------------------------------------------------------------------------------------------------
2022-04-03 09:24:00,408 EPOCH 4 done: loss 1.3799 - lr 0.020000000000000004
2022-04-03 09:24:00,408 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-03 09:25:38,645 Macro Average: 95.96	Macro avg loss: 0.48
ColumnCorpus-CONLL03FULL	95.96	
2022-04-03 09:25:38,898 ----------------------------------------------------------------------------------------------------
2022-04-03 09:25:38,899 BAD EPOCHS (no improvement): 11
2022-04-03 09:25:38,899 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-03 09:25:38,899 ==================Saving the current best model: 95.96000000000001==================
2022-04-03 09:25:46,671 ----------------------------------------------------------------------------------------------------
2022-04-03 09:25:46,680 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-03 09:25:46,842 epoch 5 - iter 0/3747 - loss 2.75201035 - samples/sec: 24.59 - decode_sents/sec: 287.61
2022-04-03 09:27:35,159 epoch 5 - iter 374/3747 - loss 4.90657184 - samples/sec: 14.54 - decode_sents/sec: 47059.50
2022-04-03 09:29:30,012 epoch 5 - iter 748/3747 - loss 5.16728609 - samples/sec: 13.70 - decode_sents/sec: 35800.89
2022-04-03 09:31:34,795 epoch 5 - iter 1122/3747 - loss 5.28192897 - samples/sec: 12.55 - decode_sents/sec: 34480.43
2022-04-03 09:33:27,720 epoch 5 - iter 1496/3747 - loss 5.34007039 - samples/sec: 13.98 - decode_sents/sec: 34564.76
2022-04-03 09:35:17,112 epoch 5 - iter 1870/3747 - loss 5.27691537 - samples/sec: 14.40 - decode_sents/sec: 27445.53
2022-04-03 09:37:06,637 epoch 5 - iter 2244/3747 - loss 5.23165422 - samples/sec: 14.41 - decode_sents/sec: 32272.34
2022-04-03 09:39:00,655 epoch 5 - iter 2618/3747 - loss 5.25151259 - samples/sec: 13.80 - decode_sents/sec: 36151.54
2022-04-03 09:40:50,980 epoch 5 - iter 2992/3747 - loss 5.22427088 - samples/sec: 14.29 - decode_sents/sec: 77113.87
2022-04-03 09:42:43,931 epoch 5 - iter 3366/3747 - loss 5.21125826 - samples/sec: 13.94 - decode_sents/sec: 56463.53
2022-04-03 09:44:33,315 epoch 5 - iter 3740/3747 - loss 5.21421743 - samples/sec: 14.41 - decode_sents/sec: 36830.15
2022-04-03 09:44:35,828 ----------------------------------------------------------------------------------------------------
2022-04-03 09:44:35,828 EPOCH 5 done: loss 1.3050 - lr 0.010000000000000002
2022-04-03 09:44:35,828 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-03 09:46:23,287 Macro Average: 96.08	Macro avg loss: 0.49
ColumnCorpus-CONLL03FULL	96.08	
2022-04-03 09:46:23,541 ----------------------------------------------------------------------------------------------------
2022-04-03 09:46:23,541 BAD EPOCHS (no improvement): 11
2022-04-03 09:46:23,541 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-03 09:46:23,541 ==================Saving the current best model: 96.08==================
2022-04-03 09:46:32,294 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2107 train
2022-04-03 09:46:32,299 loading file resources/taggers/ln_augment_data_conll03_without_clkl4/best-model.pt
[2022-04-03 09:46:35,922 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-03 09:46:35,923 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-03 09:46:36,928 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
2022-04-03 09:46:37,547 Testing using best model ...
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2138 train <torch.utils.data.dataset.ConcatDataset object at 0x7fdc7e662780>
2022-04-03 09:46:38,221 xlm-roberta-large 559890432
2022-04-03 09:46:38,221 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-03 09:47:58,796 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/ln_augment_data_conll03_without_clkl4/test.tsv
2022-04-03 09:48:15,577 0.9187	0.9309	0.9248
2022-04-03 09:48:15,578 
MICRO_AVG: acc 0.8601 - f1-score 0.9248
MACRO_AVG: acc 0.8399 - f1-score 0.9099
LOC        tp: 1556 - fp: 91 - fn: 112 - tn: 1556 - precision: 0.9447 - recall: 0.9329 - accuracy: 0.8846 - f1-score: 0.9388
MISC       tp: 588 - fp: 151 - fn: 114 - tn: 588 - precision: 0.7957 - recall: 0.8376 - accuracy: 0.6893 - f1-score: 0.8161
ORG        tp: 1543 - fp: 188 - fn: 118 - tn: 1543 - precision: 0.8914 - recall: 0.9290 - accuracy: 0.8345 - f1-score: 0.9098
PER        tp: 1571 - fp: 35 - fn: 46 - tn: 1571 - precision: 0.9782 - recall: 0.9716 - accuracy: 0.9510 - f1-score: 0.9749
2022-04-03 09:48:15,578 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2226 train
2022-04-03 09:48:15,578 ----------------------------------------------------------------------------------------------------
2022-04-03 09:48:15,578 current corpus: ColumnCorpus-CONLL03FULL
2022-04-03 09:48:15,767 xlm-roberta-large 559890432
2022-04-03 09:48:15,767 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-03 09:48:18,474 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/ln_augment_data_conll03_without_clkl4/ColumnCorpus-CONLL03FULL-test.tsv
2022-04-03 09:48:35,931 0.9187	0.9309	0.9248
2022-04-03 09:48:35,931 
MICRO_AVG: acc 0.8601 - f1-score 0.9248
MACRO_AVG: acc 0.8399 - f1-score 0.9099
LOC        tp: 1556 - fp: 91 - fn: 112 - tn: 1556 - precision: 0.9447 - recall: 0.9329 - accuracy: 0.8846 - f1-score: 0.9388
MISC       tp: 588 - fp: 151 - fn: 114 - tn: 588 - precision: 0.7957 - recall: 0.8376 - accuracy: 0.6893 - f1-score: 0.8161
ORG        tp: 1543 - fp: 188 - fn: 118 - tn: 1543 - precision: 0.8914 - recall: 0.9290 - accuracy: 0.8345 - f1-score: 0.9098
PER        tp: 1571 - fp: 35 - fn: 46 - tn: 1571 - precision: 0.9782 - recall: 0.9716 - accuracy: 0.9510 - f1-score: 0.9749

