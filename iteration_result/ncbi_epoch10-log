/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2022-04-04 11:35:36,729 Reading data from /home/miao/6207/lcx/CLNER/CLNER_datasets/ncbi_bertscore_eos_doc_full
2022-04-04 11:35:36,729 Train: /home/miao/6207/lcx/CLNER/CLNER_datasets/ncbi_bertscore_eos_doc_full/train.txt
2022-04-04 11:35:36,729 Dev: /home/miao/6207/lcx/CLNER/CLNER_datasets/ncbi_bertscore_eos_doc_full/dev.txt
2022-04-04 11:35:36,729 Test: /home/miao/6207/lcx/CLNER/CLNER_datasets/ncbi_bertscore_eos_doc_full/test.txt
2022-04-04 11:35:48,257 {b'<unk>': 0, b'O': 1, b'B-Disease': 2, b'I-Disease': 3, b'E-Disease': 4, b'S-X': 5, b'S-Disease': 6, b'<START>': 7, b'<STOP>': 8}
2022-04-04 11:35:48,257 Corpus: 5424 train + 923 dev + 940 test sentences
/home/miao/6207/lcx/CLNER/flair/utils/params.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  dict_merge.dict_merge(params_dict, yaml.load(f))
[2022-04-04 11:35:49,270 INFO] Lock 140188166130600 acquired on /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6.lock
[2022-04-04 11:35:49,271 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/config.json not found in cache or force_download set to True, downloading to /home/miao/.cache/torch/transformers/tmp7iahos0j
Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]Downloading: 100%|██████████| 289/289 [00:00<00:00, 163kB/s]
[2022-04-04 11:35:50,346 INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/config.json in cache at /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6
[2022-04-04 11:35:50,346 INFO] creating metadata file for /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6
[2022-04-04 11:35:50,348 INFO] Lock 140188166130600 released on /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6.lock
[2022-04-04 11:35:50,348 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/config.json from cache at /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6
[2022-04-04 11:35:50,349 INFO] Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 58996
}

[2022-04-04 11:35:50,349 INFO] Model name 'dmis-lab/biobert-large-cased-v1.1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'dmis-lab/biobert-large-cased-v1.1' is a path, a model identifier, or url to a directory containing tokenizer files.
[2022-04-04 11:35:51,411 INFO] Lock 140183751366864 acquired on /home/miao/.cache/torch/transformers/701732fae654e0c36bf4554c7758f748495aa3427b4084607df605f2049a89a0.b2d452d8aee26fe2e337e17013b48f3d5a81bb300c38986450d4022986348bdd.lock
[2022-04-04 11:35:51,412 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/vocab.txt not found in cache or force_download set to True, downloading to /home/miao/.cache/torch/transformers/tmp6efvb2sn
Downloading:   0%|          | 0.00/467k [00:00<?, ?B/s]Downloading:   0%|          | 2.05k/467k [00:00<00:54, 8.55kB/s]Downloading:   8%|▊         | 36.9k/467k [00:00<00:36, 11.9kB/s]Downloading:  19%|█▉        | 89.1k/467k [00:00<00:22, 16.6kB/s]Downloading:  49%|████▉     | 228k/467k [00:00<00:10, 23.5kB/s] Downloading: 100%|██████████| 467k/467k [00:00<00:00, 475kB/s] 
[2022-04-04 11:35:53,445 INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/vocab.txt in cache at /home/miao/.cache/torch/transformers/701732fae654e0c36bf4554c7758f748495aa3427b4084607df605f2049a89a0.b2d452d8aee26fe2e337e17013b48f3d5a81bb300c38986450d4022986348bdd
[2022-04-04 11:35:53,445 INFO] creating metadata file for /home/miao/.cache/torch/transformers/701732fae654e0c36bf4554c7758f748495aa3427b4084607df605f2049a89a0.b2d452d8aee26fe2e337e17013b48f3d5a81bb300c38986450d4022986348bdd
[2022-04-04 11:35:53,446 INFO] Lock 140183751366864 released on /home/miao/.cache/torch/transformers/701732fae654e0c36bf4554c7758f748495aa3427b4084607df605f2049a89a0.b2d452d8aee26fe2e337e17013b48f3d5a81bb300c38986450d4022986348bdd.lock
[2022-04-04 11:35:57,530 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/vocab.txt from cache at /home/miao/.cache/torch/transformers/701732fae654e0c36bf4554c7758f748495aa3427b4084607df605f2049a89a0.b2d452d8aee26fe2e337e17013b48f3d5a81bb300c38986450d4022986348bdd
[2022-04-04 11:35:57,530 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/added_tokens.json from cache at None
[2022-04-04 11:35:57,530 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/special_tokens_map.json from cache at None
[2022-04-04 11:35:57,530 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/tokenizer_config.json from cache at None
[2022-04-04 11:35:57,530 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/tokenizer.json from cache at None
[2022-04-04 11:35:58,625 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/dmis-lab/biobert-large-cased-v1.1/config.json from cache at /home/miao/.cache/torch/transformers/3493610bf2342adb1bf68e2a34c59b725a710eb59df1883605e40ae7e95bf9e4.5b7a692f7cc36e826065fed1096ab38064bca502b90349c26fb1b70aae2defb6
[2022-04-04 11:35:58,626 INFO] Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 58996
}

[2022-04-04 11:35:59,701 INFO] Lock 140188048679320 acquired on /home/miao/.cache/torch/transformers/8c1699719a69e0d7cccc2c016217edb876ee6732c3aa2809e15a09c70e9bc22e.2c1d459b35b7f0b1938ff35bf6334bc60282ea79ea7cf7e9656e27f726ed07c6.lock
[2022-04-04 11:35:59,702 INFO] https://cdn.huggingface.co/dmis-lab/biobert-large-cased-v1.1/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/miao/.cache/torch/transformers/tmpfmy9wm0h
Downloading:   0%|          | 0.00/1.46G [00:00<?, ?B/s]Downloading:   0%|          | 8.19k/1.46G [00:00<10:52:04, 37.4kB/s]Downloading:   0%|          | 43.0k/1.46G [00:00<8:22:53, 48.4kB/s] Downloading:   0%|          | 113k/1.46G [00:00<6:15:11, 64.9kB/s] Downloading:   0%|          | 269k/1.46G [00:00<4:32:56, 89.2kB/s]Downloading:   0%|          | 565k/1.46G [00:01<3:16:28, 124kB/s] Downloading:   0%|          | 1.16M/1.46G [00:01<2:20:12, 174kB/s]Downloading:   0%|          | 2.34M/1.46G [00:01<1:39:25, 245kB/s]Downloading:   0%|          | 4.72M/1.46G [00:01<1:10:09, 346kB/s]Downloading:   1%|          | 7.52M/1.46G [00:01<49:35, 489kB/s]  Downloading:   1%|          | 10.4M/1.46G [00:02<35:11, 687kB/s]Downloading:   1%|          | 13.4M/1.46G [00:02<25:07, 961kB/s]Downloading:   1%|          | 16.6M/1.46G [00:02<18:03, 1.33MB/s]Downloading:   1%|▏         | 19.9M/1.46G [00:02<13:05, 1.84MB/s]Downloading:   2%|▏         | 23.4M/1.46G [00:03<09:37, 2.49MB/s]Downloading:   2%|▏         | 27.1M/1.46G [00:03<07:08, 3.35MB/s]Downloading:   2%|▏         | 30.9M/1.46G [00:03<05:23, 4.43MB/s]Downloading:   2%|▏         | 34.9M/1.46G [00:03<04:09, 5.72MB/s]Downloading:   3%|▎         | 39.2M/1.46G [00:03<03:16, 7.25MB/s]Downloading:   3%|▎         | 43.6M/1.46G [00:04<02:38, 8.96MB/s]Downloading:   3%|▎         | 48.1M/1.46G [00:04<02:11, 10.8MB/s]Downloading:   4%|▎         | 52.9M/1.46G [00:04<01:51, 12.6MB/s]Downloading:   4%|▍         | 57.9M/1.46G [00:04<01:36, 14.6MB/s]Downloading:   4%|▍         | 63.1M/1.46G [00:05<01:24, 16.6MB/s]Downloading:   5%|▍         | 68.4M/1.46G [00:05<01:17, 18.1MB/s]Downloading:   5%|▌         | 73.6M/1.46G [00:05<01:11, 19.5MB/s]Downloading:   5%|▌         | 78.7M/1.46G [00:05<01:07, 20.5MB/s]Downloading:   6%|▌         | 83.9M/1.46G [00:05<01:04, 21.3MB/s]Downloading:   6%|▌         | 89.0M/1.46G [00:06<01:03, 21.7MB/s]Downloading:   6%|▋         | 94.2M/1.46G [00:06<01:01, 22.4MB/s]Downloading:   7%|▋         | 99.4M/1.46G [00:06<01:00, 22.7MB/s]Downloading:   7%|▋         | 105M/1.46G [00:06<00:59, 22.9MB/s] Downloading:   8%|▊         | 110M/1.46G [00:07<00:58, 23.0MB/s]Downloading:   8%|▊         | 115M/1.46G [00:07<00:58, 22.9MB/s]Downloading:   8%|▊         | 120M/1.46G [00:07<00:57, 23.3MB/s]Downloading:   9%|▊         | 126M/1.46G [00:07<00:57, 23.4MB/s]Downloading:   9%|▉         | 131M/1.46G [00:07<00:57, 23.3MB/s]Downloading:   9%|▉         | 136M/1.46G [00:08<00:57, 23.2MB/s]Downloading:  10%|▉         | 141M/1.46G [00:08<00:56, 23.3MB/s]Downloading:  10%|█         | 146M/1.46G [00:08<00:56, 23.4MB/s]Downloading:  10%|█         | 152M/1.46G [00:08<00:55, 23.4MB/s]Downloading:  11%|█         | 157M/1.46G [00:09<00:55, 23.5MB/s]Downloading:  11%|█         | 162M/1.46G [00:09<00:55, 23.4MB/s]Downloading:  11%|█▏        | 167M/1.46G [00:09<00:55, 23.4MB/s]Downloading:  12%|█▏        | 172M/1.46G [00:09<00:54, 23.4MB/s]Downloading:  12%|█▏        | 177M/1.46G [00:09<00:54, 23.4MB/s]Downloading:  12%|█▏        | 183M/1.46G [00:10<00:54, 23.5MB/s]Downloading:  13%|█▎        | 188M/1.46G [00:10<00:54, 23.3MB/s]Downloading:  13%|█▎        | 193M/1.46G [00:10<00:54, 23.3MB/s]Downloading:  14%|█▎        | 198M/1.46G [00:10<00:54, 23.4MB/s]Downloading:  14%|█▍        | 203M/1.46G [00:11<00:53, 23.4MB/s]Downloading:  14%|█▍        | 209M/1.46G [00:11<00:53, 23.4MB/s]Downloading:  15%|█▍        | 214M/1.46G [00:11<00:53, 23.4MB/s]Downloading:  15%|█▍        | 219M/1.46G [00:11<00:52, 23.5MB/s]Downloading:  15%|█▌        | 224M/1.46G [00:11<00:52, 23.5MB/s]Downloading:  16%|█▌        | 229M/1.46G [00:12<00:43, 28.0MB/s]Downloading:  16%|█▌        | 232M/1.46G [00:12<00:50, 24.3MB/s]Downloading:  16%|█▌        | 235M/1.46G [00:12<01:00, 20.1MB/s]Downloading:  16%|█▋        | 240M/1.46G [00:12<00:58, 20.9MB/s]Downloading:  17%|█▋        | 245M/1.46G [00:12<00:56, 21.6MB/s]Downloading:  17%|█▋        | 250M/1.46G [00:13<00:54, 22.2MB/s]Downloading:  17%|█▋        | 255M/1.46G [00:13<00:53, 22.5MB/s]Downloading:  18%|█▊        | 260M/1.46G [00:13<00:53, 22.5MB/s]Downloading:  18%|█▊        | 266M/1.46G [00:13<00:52, 22.8MB/s]Downloading:  19%|█▊        | 271M/1.46G [00:13<00:51, 23.1MB/s]Downloading:  19%|█▉        | 276M/1.46G [00:14<00:51, 23.2MB/s]Downloading:  19%|█▉        | 281M/1.46G [00:14<00:42, 27.7MB/s]Downloading:  19%|█▉        | 284M/1.46G [00:14<00:49, 23.9MB/s]Downloading:  20%|█▉        | 287M/1.46G [00:14<00:59, 19.9MB/s]Downloading:  20%|█▉        | 292M/1.46G [00:14<00:56, 20.6MB/s]Downloading:  20%|██        | 297M/1.46G [00:15<00:54, 21.4MB/s]Downloading:  21%|██        | 302M/1.46G [00:15<00:52, 22.0MB/s]Downloading:  21%|██        | 307M/1.46G [00:15<00:51, 22.5MB/s]Downloading:  21%|██▏       | 313M/1.46G [00:15<00:50, 22.8MB/s]Downloading:  22%|██▏       | 318M/1.46G [00:15<00:49, 23.0MB/s]Downloading:  22%|██▏       | 323M/1.46G [00:16<00:49, 23.2MB/s]Downloading:  22%|██▏       | 328M/1.46G [00:16<00:48, 23.3MB/s]Downloading:  23%|██▎       | 333M/1.46G [00:16<00:48, 23.4MB/s]Downloading:  23%|██▎       | 339M/1.46G [00:16<00:48, 23.2MB/s]Downloading:  24%|██▎       | 344M/1.46G [00:17<00:47, 23.6MB/s]Downloading:  24%|██▍       | 349M/1.46G [00:17<00:40, 27.2MB/s]Downloading:  24%|██▍       | 352M/1.46G [00:17<00:48, 23.1MB/s]Downloading:  24%|██▍       | 355M/1.46G [00:17<00:55, 20.0MB/s]Downloading:  25%|██▍       | 359M/1.46G [00:17<00:45, 24.1MB/s]Downloading:  25%|██▍       | 362M/1.46G [00:17<00:48, 22.6MB/s]Downloading:  25%|██▍       | 365M/1.46G [00:17<00:57, 19.1MB/s]Downloading:  25%|██▌       | 370M/1.46G [00:18<00:46, 23.3MB/s]Downloading:  25%|██▌       | 373M/1.46G [00:18<00:52, 20.6MB/s]Downloading:  26%|██▌       | 375M/1.46G [00:18<00:56, 19.2MB/s]Downloading:  26%|██▌       | 380M/1.46G [00:18<00:46, 23.2MB/s]Downloading:  26%|██▌       | 383M/1.46G [00:18<00:49, 21.9MB/s]Downloading:  26%|██▋       | 386M/1.46G [00:18<00:57, 18.8MB/s]Downloading:  27%|██▋       | 390M/1.46G [00:18<00:46, 22.9MB/s]Downloading:  27%|██▋       | 393M/1.46G [00:19<00:49, 21.7MB/s]Downloading:  27%|██▋       | 396M/1.46G [00:19<00:56, 18.8MB/s]Downloading:  27%|██▋       | 401M/1.46G [00:19<00:46, 22.9MB/s]Downloading:  28%|██▊       | 404M/1.46G [00:19<00:48, 21.9MB/s]Downloading:  28%|██▊       | 406M/1.46G [00:19<00:55, 18.9MB/s]Downloading:  28%|██▊       | 411M/1.46G [00:19<00:46, 22.8MB/s]Downloading:  28%|██▊       | 414M/1.46G [00:19<00:48, 21.6MB/s]Downloading:  28%|██▊       | 416M/1.46G [00:20<00:55, 19.0MB/s]Downloading:  29%|██▊       | 420M/1.46G [00:20<00:47, 21.9MB/s]Downloading:  29%|██▉       | 423M/1.46G [00:20<00:50, 20.7MB/s]Downloading:  29%|██▉       | 426M/1.46G [00:20<00:54, 18.9MB/s]Downloading:  29%|██▉       | 430M/1.46G [00:20<00:45, 22.6MB/s]Downloading:  30%|██▉       | 433M/1.46G [00:20<00:49, 20.9MB/s]Downloading:  30%|██▉       | 436M/1.46G [00:21<00:53, 19.1MB/s]Downloading:  30%|███       | 441M/1.46G [00:21<00:43, 23.4MB/s]Downloading:  30%|███       | 444M/1.46G [00:21<00:47, 21.5MB/s]Downloading:  31%|███       | 446M/1.46G [00:21<00:54, 18.8MB/s]Downloading:  31%|███       | 451M/1.46G [00:21<00:44, 22.6MB/s]Downloading:  31%|███       | 454M/1.46G [00:21<00:48, 20.7MB/s]Downloading:  31%|███       | 457M/1.46G [00:21<00:46, 21.8MB/s]Downloading:  31%|███▏      | 459M/1.46G [00:21<00:43, 22.9MB/s]Downloading:  32%|███▏      | 462M/1.46G [00:22<00:51, 19.6MB/s]Downloading:  32%|███▏      | 467M/1.46G [00:22<00:41, 23.8MB/s]Downloading:  32%|███▏      | 470M/1.46G [00:22<00:45, 21.7MB/s]Downloading:  32%|███▏      | 472M/1.46G [00:22<00:52, 19.0MB/s]Downloading:  33%|███▎      | 477M/1.46G [00:22<00:42, 23.1MB/s]Downloading:  33%|███▎      | 480M/1.46G [00:22<00:45, 21.3MB/s]Downloading:  33%|███▎      | 483M/1.46G [00:23<00:51, 19.0MB/s]Downloading:  33%|███▎      | 487M/1.46G [00:23<00:42, 22.8MB/s]Downloading:  34%|███▎      | 490M/1.46G [00:23<00:49, 19.6MB/s]Downloading:  34%|███▎      | 493M/1.46G [00:23<00:47, 20.2MB/s]Downloading:  34%|███▍      | 498M/1.46G [00:23<00:40, 23.9MB/s]Downloading:  34%|███▍      | 500M/1.46G [00:23<00:45, 21.1MB/s]Downloading:  34%|███▍      | 503M/1.46G [00:23<00:44, 21.8MB/s]Downloading:  35%|███▍      | 506M/1.46G [00:23<00:44, 21.5MB/s]Downloading:  35%|███▍      | 508M/1.46G [00:24<00:41, 22.8MB/s]Downloading:  35%|███▍      | 511M/1.46G [00:24<00:41, 23.0MB/s]Downloading:  35%|███▌      | 513M/1.46G [00:24<00:42, 22.5MB/s]Downloading:  35%|███▌      | 516M/1.46G [00:24<00:41, 22.6MB/s]Downloading:  35%|███▌      | 518M/1.46G [00:24<00:40, 23.0MB/s]Downloading:  36%|███▌      | 521M/1.46G [00:24<00:41, 22.9MB/s]Downloading:  36%|███▌      | 524M/1.46G [00:24<00:40, 23.2MB/s]Downloading:  36%|███▌      | 526M/1.46G [00:24<00:40, 23.0MB/s]Downloading:  36%|███▌      | 529M/1.46G [00:24<00:40, 23.2MB/s]Downloading:  36%|███▋      | 531M/1.46G [00:25<00:40, 22.9MB/s]Downloading:  37%|███▋      | 534M/1.46G [00:25<00:39, 23.5MB/s]Downloading:  37%|███▋      | 536M/1.46G [00:25<00:40, 22.7MB/s]Downloading:  37%|███▋      | 539M/1.46G [00:25<00:39, 23.5MB/s]Downloading:  37%|███▋      | 541M/1.46G [00:25<00:40, 23.0MB/s]Downloading:  37%|███▋      | 544M/1.46G [00:25<00:38, 23.7MB/s]Downloading:  37%|███▋      | 547M/1.46G [00:25<00:40, 22.8MB/s]Downloading:  38%|███▊      | 549M/1.46G [00:25<00:38, 23.7MB/s]Downloading:  38%|███▊      | 552M/1.46G [00:25<00:39, 22.9MB/s]Downloading:  38%|███▊      | 555M/1.46G [00:26<00:38, 23.7MB/s]Downloading:  38%|███▊      | 557M/1.46G [00:26<00:40, 22.2MB/s]Downloading:  38%|███▊      | 560M/1.46G [00:26<00:38, 23.7MB/s]Downloading:  38%|███▊      | 562M/1.46G [00:26<00:39, 22.9MB/s]Downloading:  39%|███▊      | 565M/1.46G [00:26<00:38, 23.4MB/s]Downloading:  39%|███▉      | 567M/1.46G [00:26<00:39, 22.7MB/s]Downloading:  39%|███▉      | 570M/1.46G [00:26<00:37, 23.8MB/s]Downloading:  39%|███▉      | 572M/1.46G [00:26<00:38, 23.2MB/s]Downloading:  39%|███▉      | 575M/1.46G [00:26<00:37, 23.6MB/s]Downloading:  40%|███▉      | 578M/1.46G [00:27<00:39, 22.3MB/s]Downloading:  40%|███▉      | 580M/1.46G [00:27<00:38, 23.1MB/s]Downloading:  40%|███▉      | 583M/1.46G [00:27<00:38, 22.8MB/s]Downloading:  40%|████      | 586M/1.46G [00:27<00:36, 23.9MB/s]Downloading:  40%|████      | 588M/1.46G [00:27<00:38, 22.9MB/s]Downloading:  40%|████      | 591M/1.46G [00:27<00:36, 23.7MB/s]Downloading:  41%|████      | 593M/1.46G [00:27<00:39, 21.8MB/s]Downloading:  41%|████      | 596M/1.46G [00:27<00:40, 21.4MB/s]Downloading:  41%|████      | 600M/1.46G [00:28<00:35, 24.4MB/s]Downloading:  41%|████      | 603M/1.46G [00:28<00:41, 20.7MB/s]Downloading:  41%|████▏     | 606M/1.46G [00:28<00:37, 22.9MB/s]Downloading:  42%|████▏     | 609M/1.46G [00:28<00:37, 22.8MB/s]Downloading:  42%|████▏     | 611M/1.46G [00:28<00:36, 23.2MB/s]Downloading:  42%|████▏     | 614M/1.46G [00:28<00:37, 22.7MB/s]Downloading:  42%|████▏     | 616M/1.46G [00:28<00:36, 23.4MB/s]Downloading:  42%|████▏     | 619M/1.46G [00:28<00:36, 23.0MB/s]Downloading:  43%|████▎     | 622M/1.46G [00:28<00:35, 23.4MB/s]Downloading:  43%|████▎     | 624M/1.46G [00:29<00:37, 22.6MB/s]Downloading:  43%|████▎     | 627M/1.46G [00:29<00:35, 23.4MB/s]Downloading:  43%|████▎     | 629M/1.46G [00:29<00:36, 22.8MB/s]Downloading:  43%|████▎     | 632M/1.46G [00:29<00:35, 23.6MB/s]Downloading:  43%|████▎     | 634M/1.46G [00:29<00:36, 22.9MB/s]Downloading:  44%|████▎     | 637M/1.46G [00:29<00:35, 23.5MB/s]Downloading:  44%|████▎     | 639M/1.46G [00:29<00:36, 22.3MB/s]Downloading:  44%|████▍     | 642M/1.46G [00:29<00:34, 23.7MB/s]Downloading:  44%|████▍     | 645M/1.46G [00:29<00:35, 23.0MB/s]Downloading:  44%|████▍     | 647M/1.46G [00:30<00:34, 23.6MB/s]Downloading:  44%|████▍     | 650M/1.46G [00:30<00:35, 22.9MB/s]Downloading:  45%|████▍     | 653M/1.46G [00:30<00:34, 23.7MB/s]Downloading:  45%|████▍     | 655M/1.46G [00:30<00:36, 22.3MB/s]Downloading:  45%|████▌     | 658M/1.46G [00:30<00:33, 23.9MB/s]Downloading:  45%|████▌     | 660M/1.46G [00:30<00:34, 23.2MB/s]Downloading:  45%|████▌     | 663M/1.46G [00:30<00:33, 23.6MB/s]Downloading:  46%|████▌     | 665M/1.46G [00:30<00:35, 22.7MB/s]Downloading:  46%|████▌     | 668M/1.46G [00:30<00:33, 23.4MB/s]Downloading:  46%|████▌     | 670M/1.46G [00:31<00:35, 22.6MB/s]Downloading:  46%|████▌     | 673M/1.46G [00:31<00:33, 23.6MB/s]Downloading:  46%|████▌     | 676M/1.46G [00:31<00:34, 22.5MB/s]Downloading:  46%|████▋     | 678M/1.46G [00:31<00:33, 23.1MB/s]Downloading:  47%|████▋     | 680M/1.46G [00:31<00:35, 22.0MB/s]Downloading:  47%|████▋     | 684M/1.46G [00:31<00:32, 23.8MB/s]Downloading:  47%|████▋     | 686M/1.46G [00:31<00:33, 23.1MB/s]Downloading:  47%|████▋     | 689M/1.46G [00:31<00:32, 23.5MB/s]Downloading:  47%|████▋     | 691M/1.46G [00:31<00:33, 22.7MB/s]Downloading:  47%|████▋     | 694M/1.46G [00:32<00:32, 23.6MB/s]Downloading:  48%|████▊     | 696M/1.46G [00:32<00:33, 22.9MB/s]Downloading:  48%|████▊     | 699M/1.46G [00:32<00:32, 23.6MB/s]Downloading:  48%|████▊     | 701M/1.46G [00:32<00:34, 22.0MB/s]Downloading:  48%|████▊     | 704M/1.46G [00:32<00:33, 22.9MB/s]Downloading:  48%|████▊     | 707M/1.46G [00:32<00:31, 23.9MB/s]Downloading:  49%|████▊     | 709M/1.46G [00:32<00:31, 23.7MB/s]Downloading:  49%|████▊     | 712M/1.46G [00:32<00:32, 23.0MB/s]Downloading:  49%|████▉     | 714M/1.46G [00:33<01:10, 10.7MB/s]Downloading:  49%|████▉     | 721M/1.46G [00:33<00:51, 14.3MB/s]Downloading:  50%|████▉     | 728M/1.46G [00:33<00:39, 18.6MB/s]Downloading:  50%|█████     | 732M/1.46G [00:33<00:37, 19.6MB/s]Downloading:  50%|█████     | 736M/1.46G [00:33<00:37, 19.3MB/s]Downloading:  51%|█████     | 740M/1.46G [00:34<00:31, 22.7MB/s]Downloading:  51%|█████     | 743M/1.46G [00:34<00:32, 22.1MB/s]Downloading:  51%|█████     | 746M/1.46G [00:34<00:34, 20.5MB/s]Downloading:  51%|█████▏    | 749M/1.46G [00:34<00:31, 22.8MB/s]Downloading:  51%|█████▏    | 752M/1.46G [00:34<00:36, 19.5MB/s]Downloading:  52%|█████▏    | 756M/1.46G [00:34<00:32, 22.0MB/s]Downloading:  52%|█████▏    | 759M/1.46G [00:34<00:29, 24.1MB/s]Downloading:  52%|█████▏    | 762M/1.46G [00:35<00:32, 21.4MB/s]Downloading:  52%|█████▏    | 766M/1.46G [00:35<00:28, 24.7MB/s]Downloading:  53%|█████▎    | 769M/1.46G [00:35<00:30, 23.0MB/s]Downloading:  53%|█████▎    | 771M/1.46G [00:35<00:29, 23.4MB/s]Downloading:  53%|█████▎    | 774M/1.46G [00:35<00:30, 22.5MB/s]Downloading:  53%|█████▎    | 776M/1.46G [00:35<00:29, 23.6MB/s]Downloading:  53%|█████▎    | 779M/1.46G [00:35<00:29, 23.0MB/s]Downloading:  53%|█████▎    | 782M/1.46G [00:35<00:28, 23.6MB/s]Downloading:  54%|█████▎    | 784M/1.46G [00:35<00:29, 22.7MB/s]Downloading:  54%|█████▍    | 787M/1.46G [00:36<00:28, 23.8MB/s]Downloading:  54%|█████▍    | 789M/1.46G [00:36<00:30, 22.4MB/s]Downloading:  54%|█████▍    | 792M/1.46G [00:36<00:28, 23.8MB/s]Downloading:  54%|█████▍    | 794M/1.46G [00:36<00:29, 22.8MB/s]Downloading:  55%|█████▍    | 797M/1.46G [00:36<00:28, 23.7MB/s]Downloading:  55%|█████▍    | 799M/1.46G [00:36<00:29, 22.4MB/s]Downloading:  55%|█████▍    | 802M/1.46G [00:36<00:28, 23.4MB/s]Downloading:  55%|█████▌    | 805M/1.46G [00:36<00:29, 22.6MB/s]Downloading:  55%|█████▌    | 807M/1.46G [00:36<00:27, 23.7MB/s]Downloading:  55%|█████▌    | 810M/1.46G [00:37<00:28, 22.5MB/s]Downloading:  56%|█████▌    | 812M/1.46G [00:37<00:30, 21.4MB/s]Downloading:  56%|█████▌    | 815M/1.46G [00:37<00:27, 23.8MB/s]Downloading:  56%|█████▌    | 818M/1.46G [00:37<00:28, 22.7MB/s]Downloading:  56%|█████▌    | 820M/1.46G [00:37<00:28, 22.8MB/s]Downloading:  56%|█████▋    | 822M/1.46G [00:37<00:28, 22.1MB/s]Downloading:  56%|█████▋    | 825M/1.46G [00:37<00:29, 21.9MB/s]Downloading:  57%|█████▋    | 828M/1.46G [00:37<00:26, 24.0MB/s]Downloading:  57%|█████▋    | 830M/1.46G [00:37<00:27, 22.8MB/s]Downloading:  57%|█████▋    | 833M/1.46G [00:38<00:26, 23.9MB/s]Downloading:  57%|█████▋    | 836M/1.46G [00:38<00:27, 22.8MB/s]Downloading:  57%|█████▋    | 838M/1.46G [00:38<00:27, 23.0MB/s]Downloading:  57%|█████▋    | 840M/1.46G [00:38<00:27, 22.3MB/s]Downloading:  58%|█████▊    | 843M/1.46G [00:38<00:25, 23.9MB/s]Downloading:  58%|█████▊    | 846M/1.46G [00:38<00:27, 22.5MB/s]Downloading:  58%|█████▊    | 849M/1.46G [00:38<00:25, 23.8MB/s]Downloading:  58%|█████▊    | 851M/1.46G [00:38<00:26, 23.2MB/s]Downloading:  58%|█████▊    | 854M/1.46G [00:38<00:25, 23.8MB/s]Downloading:  59%|█████▊    | 856M/1.46G [00:39<00:27, 22.2MB/s]Downloading:  59%|█████▉    | 859M/1.46G [00:39<00:25, 23.7MB/s]Downloading:  59%|█████▉    | 861M/1.46G [00:39<00:28, 20.9MB/s]Downloading:  59%|█████▉    | 865M/1.46G [00:39<00:26, 22.9MB/s]Downloading:  59%|█████▉    | 867M/1.46G [00:39<00:25, 23.3MB/s]Downloading:  60%|█████▉    | 870M/1.46G [00:39<00:26, 22.3MB/s]Downloading:  60%|█████▉    | 873M/1.46G [00:39<00:24, 23.7MB/s]Downloading:  60%|█████▉    | 875M/1.46G [00:39<00:25, 22.8MB/s]Downloading:  60%|██████    | 877M/1.46G [00:39<00:25, 23.1MB/s]Downloading:  60%|██████    | 880M/1.46G [00:40<00:24, 23.4MB/s]Downloading:  60%|██████    | 883M/1.46G [00:40<00:24, 23.8MB/s]Downloading:  61%|██████    | 885M/1.46G [00:40<00:24, 23.2MB/s]Downloading:  61%|██████    | 888M/1.46G [00:40<00:24, 23.2MB/s]Downloading:  61%|██████    | 890M/1.46G [00:40<00:25, 22.8MB/s]Downloading:  61%|██████    | 893M/1.46G [00:40<00:24, 23.5MB/s]Downloading:  61%|██████▏   | 896M/1.46G [00:40<00:24, 22.8MB/s]Downloading:  61%|██████▏   | 898M/1.46G [00:40<00:25, 21.8MB/s]Downloading:  62%|██████▏   | 900M/1.46G [00:40<00:25, 21.6MB/s]Downloading:  62%|██████▏   | 904M/1.46G [00:41<00:23, 24.0MB/s]Downloading:  62%|██████▏   | 906M/1.46G [00:41<00:25, 21.6MB/s]Downloading:  62%|██████▏   | 909M/1.46G [00:41<00:24, 22.9MB/s]Downloading:  62%|██████▏   | 911M/1.46G [00:41<00:23, 23.2MB/s]Downloading:  63%|██████▎   | 914M/1.46G [00:41<00:22, 24.4MB/s]Downloading:  63%|██████▎   | 917M/1.46G [00:41<00:24, 21.9MB/s]Downloading:  63%|██████▎   | 919M/1.46G [00:41<00:23, 22.8MB/s]Downloading:  63%|██████▎   | 922M/1.46G [00:41<00:23, 23.4MB/s]Downloading:  63%|██████▎   | 924M/1.46G [00:41<00:22, 23.8MB/s]Downloading:  63%|██████▎   | 926M/1.46G [00:42<00:22, 23.8MB/s]Downloading:  64%|██████▎   | 929M/1.46G [00:42<00:22, 23.5MB/s]Downloading:  64%|██████▎   | 932M/1.46G [00:42<00:22, 23.8MB/s]Downloading:  64%|██████▍   | 934M/1.46G [00:42<00:22, 23.3MB/s]Downloading:  64%|██████▍   | 937M/1.46G [00:42<00:23, 22.8MB/s]Downloading:  64%|██████▍   | 940M/1.46G [00:42<00:21, 24.0MB/s]Downloading:  64%|██████▍   | 942M/1.46G [00:42<00:22, 23.3MB/s]Downloading:  65%|██████▍   | 944M/1.46G [00:42<00:22, 23.2MB/s]Downloading:  65%|██████▍   | 947M/1.46G [00:42<00:22, 23.0MB/s]Downloading:  65%|██████▍   | 950M/1.46G [00:43<00:21, 23.4MB/s]Downloading:  65%|██████▌   | 952M/1.46G [00:43<00:21, 23.5MB/s]Downloading:  65%|██████▌   | 955M/1.46G [00:43<00:22, 22.8MB/s]Downloading:  65%|██████▌   | 957M/1.46G [00:43<00:21, 23.8MB/s]Downloading:  66%|██████▌   | 960M/1.46G [00:43<00:21, 23.3MB/s]Downloading:  66%|██████▌   | 963M/1.46G [00:43<00:21, 23.3MB/s]Downloading:  66%|██████▌   | 965M/1.46G [00:43<00:21, 23.5MB/s]Downloading:  66%|██████▌   | 968M/1.46G [00:43<00:21, 23.3MB/s]Downloading:  66%|██████▋   | 970M/1.46G [00:43<00:20, 23.5MB/s]Downloading:  67%|██████▋   | 973M/1.46G [00:44<00:20, 23.3MB/s]Downloading:  67%|██████▋   | 975M/1.46G [00:44<00:20, 23.5MB/s]Downloading:  67%|██████▋   | 978M/1.46G [00:44<00:20, 23.4MB/s]Downloading:  67%|██████▋   | 981M/1.46G [00:44<00:20, 23.4MB/s]Downloading:  67%|██████▋   | 983M/1.46G [00:44<00:20, 23.3MB/s]Downloading:  67%|██████▋   | 986M/1.46G [00:44<00:20, 23.6MB/s]Downloading:  68%|██████▊   | 988M/1.46G [00:44<00:20, 23.2MB/s]Downloading:  68%|██████▊   | 991M/1.46G [00:44<00:20, 23.2MB/s]Downloading:  68%|██████▊   | 994M/1.46G [00:44<00:20, 23.1MB/s]Downloading:  68%|██████▊   | 996M/1.46G [00:45<00:19, 23.4MB/s]Downloading:  68%|██████▊   | 999M/1.46G [00:45<00:19, 23.3MB/s]Downloading:  68%|██████▊   | 1.00G/1.46G [00:45<00:19, 23.6MB/s]Downloading:  69%|██████▊   | 1.00G/1.46G [00:45<00:19, 23.2MB/s]Downloading:  69%|██████▉   | 1.01G/1.46G [00:45<00:19, 23.4MB/s]Downloading:  69%|██████▉   | 1.01G/1.46G [00:45<00:19, 23.1MB/s]Downloading:  69%|██████▉   | 1.01G/1.46G [00:45<00:19, 23.5MB/s]Downloading:  69%|██████▉   | 1.01G/1.46G [00:45<00:19, 23.3MB/s]Downloading:  70%|██████▉   | 1.02G/1.46G [00:45<00:19, 23.4MB/s]Downloading:  70%|██████▉   | 1.02G/1.46G [00:46<00:19, 23.2MB/s]Downloading:  70%|██████▉   | 1.02G/1.46G [00:46<00:18, 23.3MB/s]Downloading:  70%|███████   | 1.02G/1.46G [00:46<00:18, 23.2MB/s]Downloading:  70%|███████   | 1.03G/1.46G [00:46<00:18, 23.3MB/s]Downloading:  70%|███████   | 1.03G/1.46G [00:46<00:18, 23.4MB/s]Downloading:  71%|███████   | 1.03G/1.46G [00:46<00:18, 23.3MB/s]Downloading:  71%|███████   | 1.03G/1.46G [00:46<00:18, 23.2MB/s]Downloading:  71%|███████   | 1.04G/1.46G [00:46<00:18, 23.5MB/s]Downloading:  71%|███████   | 1.04G/1.46G [00:46<00:18, 23.1MB/s]Downloading:  71%|███████▏  | 1.04G/1.46G [00:47<00:17, 23.6MB/s]Downloading:  71%|███████▏  | 1.05G/1.46G [00:47<00:17, 23.2MB/s]Downloading:  72%|███████▏  | 1.05G/1.46G [00:47<00:17, 23.5MB/s]Downloading:  72%|███████▏  | 1.05G/1.46G [00:47<00:17, 23.2MB/s]Downloading:  72%|███████▏  | 1.05G/1.46G [00:47<00:17, 23.5MB/s]Downloading:  72%|███████▏  | 1.06G/1.46G [00:47<00:17, 23.2MB/s]Downloading:  72%|███████▏  | 1.06G/1.46G [00:47<00:17, 23.6MB/s]Downloading:  73%|███████▎  | 1.06G/1.46G [00:47<00:17, 23.1MB/s]Downloading:  73%|███████▎  | 1.06G/1.46G [00:47<00:18, 21.9MB/s]Downloading:  73%|███████▎  | 1.07G/1.46G [00:48<00:16, 23.4MB/s]Downloading:  73%|███████▎  | 1.07G/1.46G [00:48<00:16, 23.5MB/s]Downloading:  73%|███████▎  | 1.07G/1.46G [00:48<00:16, 23.4MB/s]Downloading:  73%|███████▎  | 1.07G/1.46G [00:48<00:17, 22.1MB/s]Downloading:  74%|███████▎  | 1.08G/1.46G [00:48<00:16, 23.0MB/s]Downloading:  74%|███████▍  | 1.08G/1.46G [00:48<00:17, 22.3MB/s]Downloading:  74%|███████▍  | 1.08G/1.46G [00:48<00:15, 23.8MB/s]Downloading:  74%|███████▍  | 1.08G/1.46G [00:48<00:16, 22.5MB/s]Downloading:  74%|███████▍  | 1.09G/1.46G [00:48<00:15, 23.7MB/s]Downloading:  74%|███████▍  | 1.09G/1.46G [00:49<00:16, 22.4MB/s]Downloading:  75%|███████▍  | 1.09G/1.46G [00:49<00:15, 23.8MB/s]Downloading:  75%|███████▍  | 1.09G/1.46G [00:49<00:16, 22.6MB/s]Downloading:  75%|███████▌  | 1.10G/1.46G [00:49<00:15, 23.9MB/s]Downloading:  75%|███████▌  | 1.10G/1.46G [00:49<00:16, 22.6MB/s]Downloading:  75%|███████▌  | 1.10G/1.46G [00:49<00:15, 23.9MB/s]Downloading:  76%|███████▌  | 1.10G/1.46G [00:49<00:15, 22.6MB/s]Downloading:  76%|███████▌  | 1.11G/1.46G [00:49<00:14, 24.0MB/s]Downloading:  76%|███████▌  | 1.11G/1.46G [00:49<00:15, 22.5MB/s]Downloading:  76%|███████▌  | 1.11G/1.46G [00:50<00:14, 24.0MB/s]Downloading:  76%|███████▋  | 1.12G/1.46G [00:50<00:15, 22.6MB/s]Downloading:  76%|███████▋  | 1.12G/1.46G [00:50<00:14, 23.2MB/s]Downloading:  77%|███████▋  | 1.12G/1.46G [00:50<00:14, 22.8MB/s]Downloading:  77%|███████▋  | 1.12G/1.46G [00:50<00:14, 23.8MB/s]Downloading:  77%|███████▋  | 1.13G/1.46G [00:50<00:14, 22.5MB/s]Downloading:  77%|███████▋  | 1.13G/1.46G [00:50<00:14, 23.0MB/s]Downloading:  77%|███████▋  | 1.13G/1.46G [00:50<00:14, 22.7MB/s]Downloading:  78%|███████▊  | 1.13G/1.46G [00:50<00:13, 24.0MB/s]Downloading:  78%|███████▊  | 1.14G/1.46G [00:51<00:14, 22.8MB/s]Downloading:  78%|███████▊  | 1.14G/1.46G [00:51<00:13, 23.9MB/s]Downloading:  78%|███████▊  | 1.14G/1.46G [00:51<00:14, 22.7MB/s]Downloading:  78%|███████▊  | 1.14G/1.46G [00:51<00:13, 23.6MB/s]Downloading:  78%|███████▊  | 1.15G/1.46G [00:51<00:14, 22.5MB/s]Downloading:  79%|███████▊  | 1.15G/1.46G [00:51<00:13, 23.9MB/s]Downloading:  79%|███████▊  | 1.15G/1.46G [00:51<00:13, 22.5MB/s]Downloading:  79%|███████▉  | 1.15G/1.46G [00:51<00:12, 23.8MB/s]Downloading:  79%|███████▉  | 1.16G/1.46G [00:51<00:13, 22.5MB/s]Downloading:  79%|███████▉  | 1.16G/1.46G [00:52<00:12, 23.6MB/s]Downloading:  79%|███████▉  | 1.16G/1.46G [00:52<00:13, 22.4MB/s]Downloading:  80%|███████▉  | 1.16G/1.46G [00:52<00:12, 23.9MB/s]Downloading:  80%|███████▉  | 1.17G/1.46G [00:52<00:13, 22.6MB/s]Downloading:  80%|███████▉  | 1.17G/1.46G [00:52<00:12, 23.9MB/s]Downloading:  80%|████████  | 1.17G/1.46G [00:52<00:12, 22.5MB/s]Downloading:  80%|████████  | 1.17G/1.46G [00:52<00:11, 23.9MB/s]Downloading:  81%|████████  | 1.18G/1.46G [00:52<00:12, 22.5MB/s]Downloading:  81%|████████  | 1.18G/1.46G [00:52<00:11, 24.0MB/s]Downloading:  81%|████████  | 1.18G/1.46G [00:53<00:12, 22.2MB/s]Downloading:  81%|████████  | 1.18G/1.46G [00:53<00:11, 23.2MB/s]Downloading:  81%|████████  | 1.19G/1.46G [00:53<00:12, 22.3MB/s]Downloading:  81%|████████▏ | 1.19G/1.46G [00:53<00:11, 23.5MB/s]Downloading:  82%|████████▏ | 1.19G/1.46G [00:53<00:11, 22.4MB/s]Downloading:  82%|████████▏ | 1.20G/1.46G [00:53<00:11, 23.6MB/s]Downloading:  82%|████████▏ | 1.20G/1.46G [00:53<00:11, 22.3MB/s]Downloading:  82%|████████▏ | 1.20G/1.46G [00:53<00:10, 23.8MB/s]Downloading:  82%|████████▏ | 1.20G/1.46G [00:53<00:11, 22.5MB/s]Downloading:  82%|████████▏ | 1.21G/1.46G [00:54<00:10, 23.8MB/s]Downloading:  83%|████████▎ | 1.21G/1.46G [00:54<00:11, 22.4MB/s]Downloading:  83%|████████▎ | 1.21G/1.46G [00:54<00:10, 23.0MB/s]Downloading:  83%|████████▎ | 1.21G/1.46G [00:54<00:10, 22.7MB/s]Downloading:  83%|████████▎ | 1.22G/1.46G [00:54<00:10, 23.2MB/s]Downloading:  83%|████████▎ | 1.22G/1.46G [00:54<00:10, 22.9MB/s]Downloading:  84%|████████▎ | 1.22G/1.46G [00:54<00:09, 24.2MB/s]Downloading:  84%|████████▎ | 1.22G/1.46G [00:54<00:10, 22.6MB/s]Downloading:  84%|████████▍ | 1.23G/1.46G [00:54<00:10, 23.2MB/s]Downloading:  84%|████████▍ | 1.23G/1.46G [00:55<00:10, 22.9MB/s]Downloading:  84%|████████▍ | 1.23G/1.46G [00:55<00:09, 24.2MB/s]Downloading:  84%|████████▍ | 1.23G/1.46G [00:55<00:10, 22.7MB/s]Downloading:  85%|████████▍ | 1.24G/1.46G [00:55<00:09, 23.3MB/s]Downloading:  85%|████████▍ | 1.24G/1.46G [00:55<00:09, 22.7MB/s]Downloading:  85%|████████▍ | 1.24G/1.46G [00:55<00:09, 23.4MB/s]Downloading:  85%|████████▌ | 1.24G/1.46G [00:55<00:09, 22.9MB/s]Downloading:  85%|████████▌ | 1.25G/1.46G [00:55<00:09, 23.2MB/s]Downloading:  85%|████████▌ | 1.25G/1.46G [00:55<00:09, 23.0MB/s]Downloading:  86%|████████▌ | 1.25G/1.46G [00:56<00:08, 23.4MB/s]Downloading:  86%|████████▌ | 1.25G/1.46G [00:56<00:09, 23.0MB/s]Downloading:  86%|████████▌ | 1.26G/1.46G [00:56<00:08, 24.3MB/s]Downloading:  86%|████████▌ | 1.26G/1.46G [00:56<00:08, 22.7MB/s]Downloading:  86%|████████▋ | 1.26G/1.46G [00:56<00:08, 22.9MB/s]Downloading:  87%|████████▋ | 1.26G/1.46G [00:56<00:08, 23.1MB/s]Downloading:  87%|████████▋ | 1.27G/1.46G [00:56<00:08, 24.2MB/s]Downloading:  87%|████████▋ | 1.27G/1.46G [00:56<00:08, 22.7MB/s]Downloading:  87%|████████▋ | 1.27G/1.46G [00:56<00:08, 22.9MB/s]Downloading:  87%|████████▋ | 1.28G/1.46G [00:57<00:08, 23.1MB/s]Downloading:  87%|████████▋ | 1.28G/1.46G [00:57<00:07, 24.3MB/s]Downloading:  88%|████████▊ | 1.28G/1.46G [00:57<00:07, 22.7MB/s]Downloading:  88%|████████▊ | 1.28G/1.46G [00:57<00:07, 22.9MB/s]Downloading:  88%|████████▊ | 1.29G/1.46G [00:57<00:07, 22.9MB/s]Downloading:  88%|████████▊ | 1.29G/1.46G [00:57<00:07, 23.9MB/s]Downloading:  88%|████████▊ | 1.29G/1.46G [00:57<00:07, 22.8MB/s]Downloading:  88%|████████▊ | 1.29G/1.46G [00:57<00:07, 23.8MB/s]Downloading:  89%|████████▊ | 1.30G/1.46G [00:57<00:07, 22.8MB/s]Downloading:  89%|████████▉ | 1.30G/1.46G [00:57<00:06, 23.9MB/s]Downloading:  89%|████████▉ | 1.30G/1.46G [00:58<00:07, 22.7MB/s]Downloading:  89%|████████▉ | 1.30G/1.46G [00:58<00:06, 23.7MB/s]Downloading:  89%|████████▉ | 1.31G/1.46G [00:58<00:06, 22.8MB/s]Downloading:  90%|████████▉ | 1.31G/1.46G [00:58<00:06, 23.6MB/s]Downloading:  90%|████████▉ | 1.31G/1.46G [00:58<00:06, 23.3MB/s]Downloading:  90%|████████▉ | 1.31G/1.46G [00:58<00:06, 23.4MB/s]Downloading:  90%|█████████ | 1.32G/1.46G [00:58<00:06, 22.8MB/s]Downloading:  90%|█████████ | 1.32G/1.46G [00:58<00:06, 23.7MB/s]Downloading:  90%|█████████ | 1.32G/1.46G [00:58<00:06, 22.7MB/s]Downloading:  91%|█████████ | 1.32G/1.46G [00:59<00:05, 23.7MB/s]Downloading:  91%|█████████ | 1.33G/1.46G [00:59<00:05, 22.7MB/s]Downloading:  91%|█████████ | 1.33G/1.46G [00:59<00:05, 23.7MB/s]Downloading:  91%|█████████ | 1.33G/1.46G [00:59<00:05, 22.8MB/s]Downloading:  91%|█████████ | 1.33G/1.46G [00:59<00:05, 23.1MB/s]Downloading:  91%|█████████▏| 1.34G/1.46G [00:59<00:05, 23.1MB/s]Downloading:  92%|█████████▏| 1.34G/1.46G [00:59<00:05, 24.0MB/s]Downloading:  92%|█████████▏| 1.34G/1.46G [00:59<00:05, 22.8MB/s]Downloading:  92%|█████████▏| 1.34G/1.46G [00:59<00:04, 24.0MB/s]Downloading:  92%|█████████▏| 1.35G/1.46G [01:00<00:05, 22.7MB/s]Downloading:  92%|█████████▏| 1.35G/1.46G [01:00<00:04, 23.8MB/s]Downloading:  93%|█████████▎| 1.35G/1.46G [01:00<00:04, 22.6MB/s]Downloading:  93%|█████████▎| 1.36G/1.46G [01:00<00:04, 23.9MB/s]Downloading:  93%|█████████▎| 1.36G/1.46G [01:00<00:04, 22.5MB/s]Downloading:  93%|█████████▎| 1.36G/1.46G [01:00<00:04, 23.8MB/s]Downloading:  93%|█████████▎| 1.36G/1.46G [01:00<00:04, 22.3MB/s]Downloading:  93%|█████████▎| 1.37G/1.46G [01:00<00:04, 23.9MB/s]Downloading:  94%|█████████▎| 1.37G/1.46G [01:00<00:04, 22.8MB/s]Downloading:  94%|█████████▍| 1.37G/1.46G [01:01<00:03, 23.7MB/s]Downloading:  94%|█████████▍| 1.37G/1.46G [01:01<00:03, 22.7MB/s]Downloading:  94%|█████████▍| 1.38G/1.46G [01:01<00:03, 22.8MB/s]Downloading:  94%|█████████▍| 1.38G/1.46G [01:01<00:03, 22.8MB/s]Downloading:  94%|█████████▍| 1.38G/1.46G [01:01<00:03, 23.6MB/s]Downloading:  95%|█████████▍| 1.38G/1.46G [01:01<00:03, 24.1MB/s]Downloading:  95%|█████████▍| 1.39G/1.46G [01:01<00:03, 23.7MB/s]Downloading:  95%|█████████▍| 1.39G/1.46G [01:01<00:03, 23.3MB/s]Downloading:  95%|█████████▌| 1.39G/1.46G [01:01<00:03, 23.2MB/s]Downloading:  95%|█████████▌| 1.39G/1.46G [01:02<00:03, 22.8MB/s]Downloading:  95%|█████████▌| 1.39G/1.46G [01:02<00:02, 22.6MB/s]Downloading:  96%|█████████▌| 1.40G/1.46G [01:02<00:02, 23.3MB/s]Downloading:  96%|█████████▌| 1.40G/1.46G [01:02<00:02, 22.6MB/s]Downloading:  96%|█████████▌| 1.40G/1.46G [01:02<00:02, 23.5MB/s]Downloading:  96%|█████████▌| 1.40G/1.46G [01:02<00:02, 22.9MB/s]Downloading:  96%|█████████▋| 1.41G/1.46G [01:02<00:02, 23.8MB/s]Downloading:  96%|█████████▋| 1.41G/1.46G [01:02<00:02, 23.8MB/s]Downloading:  97%|█████████▋| 1.41G/1.46G [01:02<00:02, 23.9MB/s]Downloading:  97%|█████████▋| 1.41G/1.46G [01:02<00:02, 23.1MB/s]Downloading:  97%|█████████▋| 1.42G/1.46G [01:03<00:01, 23.8MB/s]Downloading:  97%|█████████▋| 1.42G/1.46G [01:03<00:01, 23.1MB/s]Downloading:  97%|█████████▋| 1.42G/1.46G [01:03<00:01, 23.8MB/s]Downloading:  97%|█████████▋| 1.42G/1.46G [01:03<00:01, 23.8MB/s]Downloading:  98%|█████████▊| 1.43G/1.46G [01:03<00:01, 23.8MB/s]Downloading:  98%|█████████▊| 1.43G/1.46G [01:03<00:01, 18.2MB/s]Downloading:  98%|█████████▊| 1.43G/1.46G [01:03<00:01, 21.9MB/s]Downloading:  98%|█████████▊| 1.44G/1.46G [01:03<00:01, 22.2MB/s]Downloading:  98%|█████████▊| 1.44G/1.46G [01:04<00:01, 22.6MB/s]Downloading:  99%|█████████▊| 1.44G/1.46G [01:04<00:00, 22.8MB/s]Downloading:  99%|█████████▉| 1.44G/1.46G [01:04<00:00, 23.1MB/s]Downloading:  99%|█████████▉| 1.45G/1.46G [01:04<00:00, 22.7MB/s]Downloading:  99%|█████████▉| 1.45G/1.46G [01:04<00:00, 23.0MB/s]Downloading:  99%|█████████▉| 1.45G/1.46G [01:04<00:00, 23.5MB/s]Downloading:  99%|█████████▉| 1.45G/1.46G [01:04<00:00, 23.9MB/s]Downloading: 100%|█████████▉| 1.46G/1.46G [01:04<00:00, 23.0MB/s]Downloading: 100%|█████████▉| 1.46G/1.46G [01:04<00:00, 23.5MB/s]Downloading: 100%|█████████▉| 1.46G/1.46G [01:04<00:00, 23.2MB/s]Downloading: 100%|██████████| 1.46G/1.46G [01:04<00:00, 22.5MB/s]
[2022-04-04 11:37:05,769 INFO] storing https://cdn.huggingface.co/dmis-lab/biobert-large-cased-v1.1/pytorch_model.bin in cache at /home/miao/.cache/torch/transformers/8c1699719a69e0d7cccc2c016217edb876ee6732c3aa2809e15a09c70e9bc22e.2c1d459b35b7f0b1938ff35bf6334bc60282ea79ea7cf7e9656e27f726ed07c6
[2022-04-04 11:37:05,769 INFO] creating metadata file for /home/miao/.cache/torch/transformers/8c1699719a69e0d7cccc2c016217edb876ee6732c3aa2809e15a09c70e9bc22e.2c1d459b35b7f0b1938ff35bf6334bc60282ea79ea7cf7e9656e27f726ed07c6
[2022-04-04 11:37:05,770 INFO] Lock 140188048679320 released on /home/miao/.cache/torch/transformers/8c1699719a69e0d7cccc2c016217edb876ee6732c3aa2809e15a09c70e9bc22e.2c1d459b35b7f0b1938ff35bf6334bc60282ea79ea7cf7e9656e27f726ed07c6.lock
[2022-04-04 11:37:05,770 INFO] loading weights file https://cdn.huggingface.co/dmis-lab/biobert-large-cased-v1.1/pytorch_model.bin from cache at /home/miao/.cache/torch/transformers/8c1699719a69e0d7cccc2c016217edb876ee6732c3aa2809e15a09c70e9bc22e.2c1d459b35b7f0b1938ff35bf6334bc60282ea79ea7cf7e9656e27f726ed07c6
[2022-04-04 11:37:12,043 INFO] All model checkpoint weights were used when initializing BertModel.

[2022-04-04 11:37:12,043 INFO] All the weights of BertModel were initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
2022-04-04 11:37:15,866 Model Size: 364308570
Corpus: 5424 train + 923 dev + 940 test sentences
2022-04-04 11:37:15,915 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Model: "FastSequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(58996, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (12): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (13): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (14): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (15): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (16): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (17): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (18): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (19): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (20): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (21): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (22): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (23): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.1)
  (linear): Linear(in_features=1024, out_features=9, bias=True)
)"
2022-04-04 11:37:15,918 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Corpus: "Corpus: 5424 train + 923 dev + 940 test sentences"
2022-04-04 11:37:15,918 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Parameters:
2022-04-04 11:37:15,918  - Optimizer: "AdamW"
2022-04-04 11:37:15,918  - learning_rate: "5e-06"
2022-04-04 11:37:15,918  - mini_batch_size: "1"
2022-04-04 11:37:15,918  - patience: "10"
2022-04-04 11:37:15,918  - anneal_factor: "0.5"
2022-04-04 11:37:15,918  - max_epochs: "10"
2022-04-04 11:37:15,918  - shuffle: "True"
2022-04-04 11:37:15,918  - train_with_dev: "True"
2022-04-04 11:37:15,918  - word min_freq: "-1"
2022-04-04 11:37:15,918 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Model training base path: "resources/taggers/ncbi_epoch10"
2022-04-04 11:37:15,918 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Device: cuda:0
2022-04-04 11:37:15,918 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:15,918 Embeddings storage mode: none
2022-04-04 11:37:17,212 ----------------------------------------------------------------------------------------------------
2022-04-04 11:37:17,217 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 11:37:54,924 epoch 1 - iter 0/6347 - loss 288.29376221 - samples/sec: 0.03 - decode_sents/sec: 28.89
2022-04-04 11:40:02,760 epoch 1 - iter 634/6347 - loss 21.32597481 - samples/sec: 5.26 - decode_sents/sec: 13766.62
2022-04-04 11:42:09,644 epoch 1 - iter 1268/6347 - loss 12.05124356 - samples/sec: 5.32 - decode_sents/sec: 11291.91
2022-04-04 11:44:08,475 epoch 1 - iter 1902/6347 - loss 8.66723557 - samples/sec: 5.69 - decode_sents/sec: 68173.84
2022-04-04 11:46:14,927 epoch 1 - iter 2536/6347 - loss 7.20366339 - samples/sec: 5.34 - decode_sents/sec: 11043.69
2022-04-04 11:48:22,694 epoch 1 - iter 3170/6347 - loss 6.08374531 - samples/sec: 5.29 - decode_sents/sec: 10047.79
2022-04-04 11:50:13,936 epoch 1 - iter 3804/6347 - loss 5.30986542 - samples/sec: 6.07 - decode_sents/sec: 20494.39
2022-04-04 11:52:18,335 epoch 1 - iter 4438/6347 - loss 4.78573472 - samples/sec: 5.43 - decode_sents/sec: 17177.33
2022-04-04 11:54:25,326 epoch 1 - iter 5072/6347 - loss 4.38668403 - samples/sec: 5.33 - decode_sents/sec: 17691.83
2022-04-04 11:56:20,684 epoch 1 - iter 5706/6347 - loss 4.08265285 - samples/sec: 5.85 - decode_sents/sec: 28031.59
2022-04-04 11:58:27,844 epoch 1 - iter 6340/6347 - loss 3.83148015 - samples/sec: 5.31 - decode_sents/sec: 31487.20
2022-04-04 11:58:29,019 ----------------------------------------------------------------------------------------------------
2022-04-04 11:58:29,019 EPOCH 1 done: loss 0.9573 - lr 0.05
2022-04-04 11:58:29,019 ----------------------------------------------------------------------------------------------------
2022-04-04 11:58:29,019 ----------------------------------------------------------------------------------------------------
2022-04-04 11:58:29,019 BAD EPOCHS (no improvement): 11
2022-04-04 11:58:29,019 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 11:58:29,020 ----------------------------------------------------------------------------------------------------
2022-04-04 11:58:29,024 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 11:58:29,270 epoch 2 - iter 0/6347 - loss 0.01147461 - samples/sec: 4.07 - decode_sents/sec: 24.51
2022-04-04 12:00:40,661 epoch 2 - iter 634/6347 - loss 1.13807414 - samples/sec: 5.11 - decode_sents/sec: 11523.96
2022-04-04 12:02:44,597 epoch 2 - iter 1268/6347 - loss 1.68749034 - samples/sec: 5.46 - decode_sents/sec: 18592.73
2022-04-04 12:04:50,458 epoch 2 - iter 1902/6347 - loss 1.45515707 - samples/sec: 5.36 - decode_sents/sec: 21931.45
2022-04-04 12:06:36,632 epoch 2 - iter 2536/6347 - loss 1.34364145 - samples/sec: 6.36 - decode_sents/sec: 29449.03
2022-04-04 12:08:14,149 epoch 2 - iter 3170/6347 - loss 1.24133108 - samples/sec: 6.94 - decode_sents/sec: 25459.45
2022-04-04 12:09:52,499 epoch 2 - iter 3804/6347 - loss 1.34634097 - samples/sec: 6.88 - decode_sents/sec: 24494.88
2022-04-04 12:11:32,530 epoch 2 - iter 4438/6347 - loss 1.26589746 - samples/sec: 6.79 - decode_sents/sec: 34879.64
2022-04-04 12:13:11,053 epoch 2 - iter 5072/6347 - loss 1.20577242 - samples/sec: 6.90 - decode_sents/sec: 14607.72
2022-04-04 12:14:52,401 epoch 2 - iter 5706/6347 - loss 1.17018969 - samples/sec: 6.70 - decode_sents/sec: 19445.05
2022-04-04 12:16:30,076 epoch 2 - iter 6340/6347 - loss 1.13774878 - samples/sec: 6.95 - decode_sents/sec: 43398.32
2022-04-04 12:16:31,075 ----------------------------------------------------------------------------------------------------
2022-04-04 12:16:31,075 EPOCH 2 done: loss 0.2845 - lr 0.045000000000000005
2022-04-04 12:16:31,075 ----------------------------------------------------------------------------------------------------
2022-04-04 12:16:31,075 ----------------------------------------------------------------------------------------------------
2022-04-04 12:16:31,075 BAD EPOCHS (no improvement): 11
2022-04-04 12:16:31,075 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 12:16:31,075 ----------------------------------------------------------------------------------------------------
2022-04-04 12:16:31,080 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 12:16:31,265 epoch 3 - iter 0/6347 - loss 0.23291016 - samples/sec: 5.42 - decode_sents/sec: 25.68
2022-04-04 12:18:11,659 epoch 3 - iter 634/6347 - loss 1.90465707 - samples/sec: 6.76 - decode_sents/sec: 16536.63
2022-04-04 12:19:53,539 epoch 3 - iter 1268/6347 - loss 1.28253283 - samples/sec: 6.67 - decode_sents/sec: 17360.80
2022-04-04 12:21:33,961 epoch 3 - iter 1902/6347 - loss 1.23897666 - samples/sec: 6.76 - decode_sents/sec: 10093.71
2022-04-04 12:23:25,294 epoch 3 - iter 2536/6347 - loss 1.12569862 - samples/sec: 6.06 - decode_sents/sec: 16069.55
2022-04-04 12:25:16,625 epoch 3 - iter 3170/6347 - loss 1.01464900 - samples/sec: 6.04 - decode_sents/sec: 26464.86
2022-04-04 12:27:11,291 epoch 3 - iter 3804/6347 - loss 0.94531503 - samples/sec: 5.89 - decode_sents/sec: 26169.25
2022-04-04 12:29:04,803 epoch 3 - iter 4438/6347 - loss 0.92189353 - samples/sec: 5.95 - decode_sents/sec: 16042.40
2022-04-04 12:31:00,565 epoch 3 - iter 5072/6347 - loss 0.90212075 - samples/sec: 5.83 - decode_sents/sec: 22056.79
2022-04-04 12:32:54,832 epoch 3 - iter 5706/6347 - loss 0.89957916 - samples/sec: 5.91 - decode_sents/sec: 43844.83
2022-04-04 12:34:45,727 epoch 3 - iter 6340/6347 - loss 0.88366676 - samples/sec: 6.07 - decode_sents/sec: 21002.49
2022-04-04 12:34:46,942 ----------------------------------------------------------------------------------------------------
2022-04-04 12:34:46,942 EPOCH 3 done: loss 0.2208 - lr 0.04000000000000001
2022-04-04 12:34:46,942 ----------------------------------------------------------------------------------------------------
2022-04-04 12:34:46,942 ----------------------------------------------------------------------------------------------------
2022-04-04 12:34:46,942 BAD EPOCHS (no improvement): 11
2022-04-04 12:34:46,942 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 12:34:46,942 ----------------------------------------------------------------------------------------------------
2022-04-04 12:34:46,947 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 12:34:47,114 epoch 4 - iter 0/6347 - loss 1.59729004 - samples/sec: 5.97 - decode_sents/sec: 36.28
2022-04-04 12:36:38,194 epoch 4 - iter 634/6347 - loss 0.61653472 - samples/sec: 6.06 - decode_sents/sec: 15742.76
2022-04-04 12:38:35,236 epoch 4 - iter 1268/6347 - loss 0.53912131 - samples/sec: 5.77 - decode_sents/sec: 30874.85
2022-04-04 12:40:20,270 epoch 4 - iter 1902/6347 - loss 0.53913258 - samples/sec: 6.48 - decode_sents/sec: 26852.36
2022-04-04 12:42:10,963 epoch 4 - iter 2536/6347 - loss 0.54670524 - samples/sec: 6.08 - decode_sents/sec: 21561.40
2022-04-04 12:44:00,955 epoch 4 - iter 3170/6347 - loss 0.55180903 - samples/sec: 6.12 - decode_sents/sec: 17172.79
2022-04-04 12:45:57,268 epoch 4 - iter 3804/6347 - loss 0.56652783 - samples/sec: 5.80 - decode_sents/sec: 21599.40
2022-04-04 12:47:42,315 epoch 4 - iter 4438/6347 - loss 0.55729187 - samples/sec: 6.44 - decode_sents/sec: 17464.79
2022-04-04 12:49:23,686 epoch 4 - iter 5072/6347 - loss 0.55968552 - samples/sec: 6.71 - decode_sents/sec: 26387.39
2022-04-04 12:51:01,414 epoch 4 - iter 5706/6347 - loss 0.59984141 - samples/sec: 6.94 - decode_sents/sec: 26984.24
2022-04-04 12:52:40,626 epoch 4 - iter 6340/6347 - loss 0.72778954 - samples/sec: 6.85 - decode_sents/sec: 21659.92
2022-04-04 12:52:41,570 ----------------------------------------------------------------------------------------------------
2022-04-04 12:52:41,571 EPOCH 4 done: loss 0.1818 - lr 0.034999999999999996
2022-04-04 12:52:41,571 ----------------------------------------------------------------------------------------------------
2022-04-04 12:52:41,571 ----------------------------------------------------------------------------------------------------
2022-04-04 12:52:41,571 BAD EPOCHS (no improvement): 11
2022-04-04 12:52:41,571 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 12:52:41,571 ----------------------------------------------------------------------------------------------------
2022-04-04 12:52:41,576 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 12:52:41,701 epoch 5 - iter 0/6347 - loss 2.01934814 - samples/sec: 7.95 - decode_sents/sec: 43.96
2022-04-04 12:54:26,211 epoch 5 - iter 634/6347 - loss 0.47804014 - samples/sec: 6.53 - decode_sents/sec: 40628.70
2022-04-04 12:56:01,879 epoch 5 - iter 1268/6347 - loss 0.47162010 - samples/sec: 7.10 - decode_sents/sec: 25167.89
2022-04-04 12:57:40,366 epoch 5 - iter 1902/6347 - loss 0.45438780 - samples/sec: 6.89 - decode_sents/sec: 15939.51
2022-04-04 12:59:22,106 epoch 5 - iter 2536/6347 - loss 0.45591695 - samples/sec: 6.69 - decode_sents/sec: 41125.08
2022-04-04 13:01:05,297 epoch 5 - iter 3170/6347 - loss 0.45596022 - samples/sec: 6.59 - decode_sents/sec: 46519.40
2022-04-04 13:02:44,701 epoch 5 - iter 3804/6347 - loss 0.47663941 - samples/sec: 6.84 - decode_sents/sec: 12133.38
2022-04-04 13:04:24,743 epoch 5 - iter 4438/6347 - loss 0.46991091 - samples/sec: 6.80 - decode_sents/sec: 92377.85
2022-04-04 13:06:07,264 epoch 5 - iter 5072/6347 - loss 0.60377330 - samples/sec: 6.64 - decode_sents/sec: 17017.61
2022-04-04 13:07:48,476 epoch 5 - iter 5706/6347 - loss 0.58931857 - samples/sec: 6.72 - decode_sents/sec: 109242.82
2022-04-04 13:09:35,468 epoch 5 - iter 6340/6347 - loss 0.58968728 - samples/sec: 6.34 - decode_sents/sec: 24444.44
2022-04-04 13:09:36,539 ----------------------------------------------------------------------------------------------------
2022-04-04 13:09:36,540 EPOCH 5 done: loss 0.1479 - lr 0.03
2022-04-04 13:09:36,540 ----------------------------------------------------------------------------------------------------
2022-04-04 13:09:36,540 ----------------------------------------------------------------------------------------------------
2022-04-04 13:09:36,540 BAD EPOCHS (no improvement): 11
2022-04-04 13:09:36,540 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 13:09:36,540 ----------------------------------------------------------------------------------------------------
2022-04-04 13:09:36,544 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 13:09:36,724 epoch 6 - iter 0/6347 - loss 0.03051758 - samples/sec: 5.56 - decode_sents/sec: 33.54
2022-04-04 13:11:29,361 epoch 6 - iter 634/6347 - loss 1.13281639 - samples/sec: 6.01 - decode_sents/sec: 18500.76
2022-04-04 13:13:29,510 epoch 6 - iter 1268/6347 - loss 0.77782370 - samples/sec: 5.63 - decode_sents/sec: 29247.24
2022-04-04 13:15:27,744 epoch 6 - iter 1902/6347 - loss 0.67077390 - samples/sec: 5.71 - decode_sents/sec: 20519.70
2022-04-04 13:17:22,162 epoch 6 - iter 2536/6347 - loss 0.60557735 - samples/sec: 5.89 - decode_sents/sec: 23160.03
2022-04-04 13:19:15,623 epoch 6 - iter 3170/6347 - loss 0.58667983 - samples/sec: 5.94 - decode_sents/sec: 20664.00
2022-04-04 13:21:09,043 epoch 6 - iter 3804/6347 - loss 0.56792021 - samples/sec: 5.96 - decode_sents/sec: 45527.04
2022-04-04 13:22:57,655 epoch 6 - iter 4438/6347 - loss 0.56197066 - samples/sec: 6.24 - decode_sents/sec: 23041.03
2022-04-04 13:24:43,135 epoch 6 - iter 5072/6347 - loss 0.53911128 - samples/sec: 6.44 - decode_sents/sec: 22229.93
2022-04-04 13:26:37,326 epoch 6 - iter 5706/6347 - loss 0.51627252 - samples/sec: 5.92 - decode_sents/sec: 15914.14
2022-04-04 13:28:27,216 epoch 6 - iter 6340/6347 - loss 0.50186375 - samples/sec: 6.14 - decode_sents/sec: 22902.32
2022-04-04 13:28:28,388 ----------------------------------------------------------------------------------------------------
2022-04-04 13:28:28,388 EPOCH 6 done: loss 0.1258 - lr 0.025
2022-04-04 13:28:28,388 ----------------------------------------------------------------------------------------------------
2022-04-04 13:28:28,388 ----------------------------------------------------------------------------------------------------
2022-04-04 13:28:28,388 BAD EPOCHS (no improvement): 11
2022-04-04 13:28:28,388 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 13:28:28,389 ----------------------------------------------------------------------------------------------------
2022-04-04 13:28:28,393 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 13:28:28,524 epoch 7 - iter 0/6347 - loss 0.01245117 - samples/sec: 7.61 - decode_sents/sec: 54.77
2022-04-04 13:30:21,784 epoch 7 - iter 634/6347 - loss 0.43815620 - samples/sec: 5.95 - decode_sents/sec: 19323.67
2022-04-04 13:32:12,998 epoch 7 - iter 1268/6347 - loss 0.42386182 - samples/sec: 6.07 - decode_sents/sec: 39016.21
2022-04-04 13:34:04,672 epoch 7 - iter 1902/6347 - loss 0.42994390 - samples/sec: 6.05 - decode_sents/sec: 16716.15
2022-04-04 13:36:00,598 epoch 7 - iter 2536/6347 - loss 0.42970644 - samples/sec: 5.82 - decode_sents/sec: 22681.97
2022-04-04 13:37:55,722 epoch 7 - iter 3170/6347 - loss 0.41475004 - samples/sec: 5.86 - decode_sents/sec: 19727.50
2022-04-04 13:39:54,300 epoch 7 - iter 3804/6347 - loss 0.41097401 - samples/sec: 5.69 - decode_sents/sec: 21372.16
2022-04-04 13:41:39,354 epoch 7 - iter 4438/6347 - loss 0.54920742 - samples/sec: 6.45 - decode_sents/sec: 45502.89
2022-04-04 13:43:20,056 epoch 7 - iter 5072/6347 - loss 0.53699292 - samples/sec: 6.76 - decode_sents/sec: 28640.24
2022-04-04 13:44:57,679 epoch 7 - iter 5706/6347 - loss 0.52368574 - samples/sec: 6.96 - decode_sents/sec: 25523.72
2022-04-04 13:46:37,188 epoch 7 - iter 6340/6347 - loss 0.51243941 - samples/sec: 6.86 - decode_sents/sec: 22434.73
2022-04-04 13:46:38,124 ----------------------------------------------------------------------------------------------------
2022-04-04 13:46:38,124 EPOCH 7 done: loss 0.1281 - lr 0.020000000000000004
2022-04-04 13:46:38,124 ----------------------------------------------------------------------------------------------------
2022-04-04 13:46:38,124 ----------------------------------------------------------------------------------------------------
2022-04-04 13:46:38,124 BAD EPOCHS (no improvement): 11
2022-04-04 13:46:38,124 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 13:46:38,124 ----------------------------------------------------------------------------------------------------
2022-04-04 13:46:38,129 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 13:46:38,300 epoch 8 - iter 0/6347 - loss 0.02099609 - samples/sec: 5.84 - decode_sents/sec: 28.37
2022-04-04 13:48:16,084 epoch 8 - iter 634/6347 - loss 0.33511690 - samples/sec: 6.95 - decode_sents/sec: 34004.54
2022-04-04 13:50:01,520 epoch 8 - iter 1268/6347 - loss 0.35044584 - samples/sec: 6.47 - decode_sents/sec: 24862.69
2022-04-04 13:51:38,690 epoch 8 - iter 1902/6347 - loss 0.35726453 - samples/sec: 6.98 - decode_sents/sec: 24118.53
2022-04-04 13:53:15,123 epoch 8 - iter 2536/6347 - loss 0.38587104 - samples/sec: 7.04 - decode_sents/sec: 29404.42
2022-04-04 13:55:00,831 epoch 8 - iter 3170/6347 - loss 0.38050399 - samples/sec: 6.43 - decode_sents/sec: 30286.20
2022-04-04 13:56:53,075 epoch 8 - iter 3804/6347 - loss 0.37494879 - samples/sec: 6.00 - decode_sents/sec: 48243.63
2022-04-04 13:58:46,299 epoch 8 - iter 4438/6347 - loss 0.47961646 - samples/sec: 5.96 - decode_sents/sec: 42336.35
2022-04-04 14:00:40,332 epoch 8 - iter 5072/6347 - loss 0.46896093 - samples/sec: 5.92 - decode_sents/sec: 28033.07
2022-04-04 14:02:22,587 epoch 8 - iter 5706/6347 - loss 0.46355322 - samples/sec: 6.63 - decode_sents/sec: 19681.51
2022-04-04 14:04:14,692 epoch 8 - iter 6340/6347 - loss 0.46081224 - samples/sec: 6.03 - decode_sents/sec: 17190.55
2022-04-04 14:04:15,522 ----------------------------------------------------------------------------------------------------
2022-04-04 14:04:15,522 EPOCH 8 done: loss 0.1153 - lr 0.015
2022-04-04 14:04:15,522 ----------------------------------------------------------------------------------------------------
2022-04-04 14:04:15,522 ----------------------------------------------------------------------------------------------------
2022-04-04 14:04:15,522 BAD EPOCHS (no improvement): 11
2022-04-04 14:04:15,522 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 14:04:15,522 ----------------------------------------------------------------------------------------------------
2022-04-04 14:04:15,527 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 14:04:15,610 epoch 9 - iter 0/6347 - loss 0.00018311 - samples/sec: 11.98 - decode_sents/sec: 91.49
2022-04-04 14:05:51,857 epoch 9 - iter 634/6347 - loss 0.35650015 - samples/sec: 7.06 - decode_sents/sec: 26615.84
2022-04-04 14:07:26,518 epoch 9 - iter 1268/6347 - loss 0.37758909 - samples/sec: 7.20 - decode_sents/sec: 23390.03
2022-04-04 14:09:05,921 epoch 9 - iter 1902/6347 - loss 0.37505583 - samples/sec: 6.82 - decode_sents/sec: 15926.53
2022-04-04 14:10:49,602 epoch 9 - iter 2536/6347 - loss 0.44730910 - samples/sec: 6.56 - decode_sents/sec: 23056.42
2022-04-04 14:12:32,604 epoch 9 - iter 3170/6347 - loss 0.42651613 - samples/sec: 6.61 - decode_sents/sec: 38409.27
2022-04-04 14:14:18,679 epoch 9 - iter 3804/6347 - loss 0.41884980 - samples/sec: 6.40 - decode_sents/sec: 30058.54
2022-04-04 14:16:10,445 epoch 9 - iter 4438/6347 - loss 0.41295828 - samples/sec: 6.04 - decode_sents/sec: 17610.29
2022-04-04 14:18:02,376 epoch 9 - iter 5072/6347 - loss 0.40423003 - samples/sec: 6.04 - decode_sents/sec: 36706.31
2022-04-04 14:19:39,960 epoch 9 - iter 5706/6347 - loss 0.39759938 - samples/sec: 6.97 - decode_sents/sec: 20797.33
2022-04-04 14:21:19,266 epoch 9 - iter 6340/6347 - loss 0.39413134 - samples/sec: 6.85 - decode_sents/sec: 27472.94
2022-04-04 14:21:20,172 ----------------------------------------------------------------------------------------------------
2022-04-04 14:21:20,172 EPOCH 9 done: loss 0.0985 - lr 0.010000000000000002
2022-04-04 14:21:20,172 ----------------------------------------------------------------------------------------------------
2022-04-04 14:21:20,172 ----------------------------------------------------------------------------------------------------
2022-04-04 14:21:20,172 BAD EPOCHS (no improvement): 11
2022-04-04 14:21:20,172 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 14:21:20,172 ----------------------------------------------------------------------------------------------------
2022-04-04 14:21:20,177 Current loss interpolation: 1
['dmis-lab/biobert-large-cased-v1.1_v2doc']
2022-04-04 14:21:20,334 epoch 10 - iter 0/6347 - loss 0.05212402 - samples/sec: 6.37 - decode_sents/sec: 30.67
2022-04-04 14:23:01,891 epoch 10 - iter 634/6347 - loss 0.39010510 - samples/sec: 6.71 - decode_sents/sec: 17918.58
2022-04-04 14:24:58,053 epoch 10 - iter 1268/6347 - loss 0.37248744 - samples/sec: 5.82 - decode_sents/sec: 16242.99
2022-04-04 14:26:56,137 epoch 10 - iter 1902/6347 - loss 0.34347858 - samples/sec: 5.70 - decode_sents/sec: 30540.11
2022-04-04 14:28:34,887 epoch 10 - iter 2536/6347 - loss 0.39154239 - samples/sec: 6.88 - decode_sents/sec: 16171.97
2022-04-04 14:30:18,721 epoch 10 - iter 3170/6347 - loss 0.37892922 - samples/sec: 6.55 - decode_sents/sec: 21105.85
2022-04-04 14:32:02,090 epoch 10 - iter 3804/6347 - loss 0.38081150 - samples/sec: 6.59 - decode_sents/sec: 14151.85
2022-04-04 14:33:50,865 epoch 10 - iter 4438/6347 - loss 0.37901370 - samples/sec: 6.20 - decode_sents/sec: 63469.67
2022-04-04 14:35:40,809 epoch 10 - iter 5072/6347 - loss 0.37146263 - samples/sec: 6.18 - decode_sents/sec: 19201.44
2022-04-04 14:37:28,479 epoch 10 - iter 5706/6347 - loss 0.37362936 - samples/sec: 6.31 - decode_sents/sec: 28535.74
2022-04-04 14:39:22,707 epoch 10 - iter 6340/6347 - loss 0.36706885 - samples/sec: 5.92 - decode_sents/sec: 18403.20
2022-04-04 14:39:23,873 ----------------------------------------------------------------------------------------------------
2022-04-04 14:39:23,873 EPOCH 10 done: loss 0.0917 - lr 0.005000000000000001
2022-04-04 14:39:23,873 ----------------------------------------------------------------------------------------------------
2022-04-04 14:39:23,873 ----------------------------------------------------------------------------------------------------
2022-04-04 14:39:23,873 BAD EPOCHS (no improvement): 11
2022-04-04 14:39:23,873 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 14:39:25,744 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2107 train
2022-04-04 14:39:25,746 loading file resources/taggers/ncbi_epoch10/final-model.pt
2022-04-04 14:39:28,404 Testing using final model ...
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2138 train <torch.utils.data.dataset.ConcatDataset object at 0x7f7f251795c0>
2022-04-04 14:39:28,651 dmis-lab/biobert-large-cased-v1.1_v2doc 364299264
2022-04-04 14:39:28,651 first
Traceback (most recent call last):
  File "train.py", line 362, in <module>
    getattr(trainer,'train')(**train_config)
  File "/home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py", line 1291, in train
    final_score = self.final_test(base_path, eval_mini_batch_size, num_workers)
  File "/home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py", line 2147, in final_test
    self.gpu_friendly_assign_embedding([loader],gen=gen)
  File "/home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py", line 1173, in gpu_friendly_assign_embedding
    embedding.embed(sentences)
  File "/home/miao/6207/lcx/CLNER/flair/embeddings.py", line 99, in embed
    self._add_embeddings_internal(sentences)
  File "/home/miao/6207/lcx/CLNER/flair/embeddings.py", line 3023, in _add_embeddings_internal
    model_max_length = self.tokenizer.model_max_length-2
AttributeError: 'NoneType' object has no attribute 'model_max_length'
