/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2022-04-04 04:20:27,395 Reading data from /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4
2022-04-04 04:20:27,396 Train: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/train.txt
2022-04-04 04:20:27,396 Dev: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/dev.txt
2022-04-04 04:20:27,396 Test: /home/miao/6207/lcx/CLNER/CLNER_datasets/conll_03_bertscore_eos_doc_full_iter4/test.txt
2022-04-04 04:21:05,063 {b'<unk>': 0, b'O': 1, b'S-ORG': 2, b'S-MISC': 3, b'S-X': 4, b'B-PER': 5, b'E-PER': 6, b'S-LOC': 7, b'B-ORG': 8, b'E-ORG': 9, b'I-PER': 10, b'S-PER': 11, b'B-MISC': 12, b'I-MISC': 13, b'E-MISC': 14, b'I-ORG': 15, b'B-LOC': 16, b'E-LOC': 17, b'I-LOC': 18, b'<START>': 19, b'<STOP>': 20}
2022-04-04 04:21:05,064 Corpus: 14987 train + 3466 dev + 3684 test sentences
/home/miao/6207/lcx/CLNER/flair/utils/params.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  dict_merge.dict_merge(params_dict, yaml.load(f))
[2022-04-04 04:21:06,105 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-04 04:21:06,106 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-04 04:21:07,068 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
[2022-04-04 04:21:08,529 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-04 04:21:08,530 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-04 04:21:08,630 INFO] loading weights file https://cdn.huggingface.co/xlm-roberta-large-pytorch_model.bin from cache at /home/miao/.cache/torch/transformers/a89d1c4637c1ea5ecd460c2a7c06a03acc9a961fc8c59aa2dd76d8a7f1e94536.2f41fe28a80f2730715b795242a01fc3dda846a85e7903adb3907dc5c5a498bf
[2022-04-04 04:21:22,449 INFO] All model checkpoint weights were used when initializing XLMRobertaModel.

[2022-04-04 04:21:22,449 INFO] All the weights of XLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use XLMRobertaModel for predictions without further training.
2022-04-04 04:21:25,798 Model Size: 559912398
Corpus: 14987 train + 3466 dev + 3684 test sentences
2022-04-04 04:21:25,846 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,848 Model: "FastSequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): TransformerWordEmbeddings(
      (model): XLMRobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(250002, 1024, padding_idx=1)
          (position_embeddings): Embedding(514, 1024, padding_idx=1)
          (token_type_embeddings): Embedding(1, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (12): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (13): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (14): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (15): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (16): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (17): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (18): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (19): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (20): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (21): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (22): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (23): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.1)
  (linear): Linear(in_features=1024, out_features=21, bias=True)
)"
2022-04-04 04:21:25,848 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,848 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2022-04-04 04:21:25,848 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,848 Parameters:
2022-04-04 04:21:25,848  - Optimizer: "AdamW"
2022-04-04 04:21:25,848  - learning_rate: "5e-06"
2022-04-04 04:21:25,848  - mini_batch_size: "4"
2022-04-04 04:21:25,848  - patience: "10"
2022-04-04 04:21:25,848  - anneal_factor: "0.5"
2022-04-04 04:21:25,848  - max_epochs: "10"
2022-04-04 04:21:25,849  - shuffle: "True"
2022-04-04 04:21:25,849  - train_with_dev: "False"
2022-04-04 04:21:25,849  - word min_freq: "-1"
2022-04-04 04:21:25,849 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,849 Model training base path: "resources/taggers/ln_augment_data_conll03_epoch104"
2022-04-04 04:21:25,849 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,849 Device: cuda:0
2022-04-04 04:21:25,849 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:25,849 Embeddings storage mode: none
2022-04-04 04:21:29,336 ----------------------------------------------------------------------------------------------------
2022-04-04 04:21:29,339 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 04:21:30,218 epoch 1 - iter 0/3747 - loss 744.18151855 - samples/sec: 4.56 - decode_sents/sec: 105.95
2022-04-04 04:23:13,491 epoch 1 - iter 374/3747 - loss 152.40127232 - samples/sec: 15.27 - decode_sents/sec: 29148.22
2022-04-04 04:24:59,877 epoch 1 - iter 748/3747 - loss 86.44981677 - samples/sec: 14.80 - decode_sents/sec: 61747.71
2022-04-04 04:26:42,732 epoch 1 - iter 1122/3747 - loss 61.78282093 - samples/sec: 15.27 - decode_sents/sec: 81364.65
2022-04-04 04:28:21,611 epoch 1 - iter 1496/3747 - loss 48.98842740 - samples/sec: 15.91 - decode_sents/sec: 30103.77
2022-04-04 04:30:12,800 epoch 1 - iter 1870/3747 - loss 41.19415713 - samples/sec: 14.14 - decode_sents/sec: 122979.87
2022-04-04 04:31:58,601 epoch 1 - iter 2244/3747 - loss 35.84691302 - samples/sec: 14.81 - decode_sents/sec: 35874.59
2022-04-04 04:33:43,907 epoch 1 - iter 2618/3747 - loss 31.96457759 - samples/sec: 14.89 - decode_sents/sec: 96703.12
2022-04-04 04:35:30,904 epoch 1 - iter 2992/3747 - loss 29.04709990 - samples/sec: 14.65 - decode_sents/sec: 146194.75
2022-04-04 04:37:14,515 epoch 1 - iter 3366/3747 - loss 26.74360481 - samples/sec: 15.15 - decode_sents/sec: 40312.49
2022-04-04 04:38:58,266 epoch 1 - iter 3740/3747 - loss 24.87202677 - samples/sec: 15.13 - decode_sents/sec: 41516.77
2022-04-04 04:39:00,173 ----------------------------------------------------------------------------------------------------
2022-04-04 04:39:00,173 EPOCH 1 done: loss 6.2122 - lr 0.05
2022-04-04 04:39:00,173 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 04:40:28,968 Macro Average: 94.97	Macro avg loss: 0.50
ColumnCorpus-CONLL03FULL	94.97	
2022-04-04 04:40:29,215 ----------------------------------------------------------------------------------------------------
2022-04-04 04:40:29,215 BAD EPOCHS (no improvement): 11
2022-04-04 04:40:29,215 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 04:40:29,215 ==================Saving the current best model: 94.97==================
2022-04-04 04:40:32,366 ----------------------------------------------------------------------------------------------------
2022-04-04 04:40:32,369 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 04:40:32,793 epoch 2 - iter 0/3747 - loss 8.39126587 - samples/sec: 9.44 - decode_sents/sec: 100.35
2022-04-04 04:42:16,521 epoch 2 - iter 374/3747 - loss 7.16856713 - samples/sec: 15.12 - decode_sents/sec: 48911.64
2022-04-04 04:43:52,297 epoch 2 - iter 748/3747 - loss 7.00380627 - samples/sec: 16.38 - decode_sents/sec: 43645.69
2022-04-04 04:45:28,962 epoch 2 - iter 1122/3747 - loss 7.09872543 - samples/sec: 16.25 - decode_sents/sec: 34283.93
2022-04-04 04:47:14,490 epoch 2 - iter 1496/3747 - loss 7.07919900 - samples/sec: 14.85 - decode_sents/sec: 52752.55
2022-04-04 04:48:58,862 epoch 2 - iter 1870/3747 - loss 7.03351626 - samples/sec: 15.11 - decode_sents/sec: 31831.45
2022-04-04 04:50:46,374 epoch 2 - iter 2244/3747 - loss 7.06676447 - samples/sec: 14.65 - decode_sents/sec: 45313.05
2022-04-04 04:52:37,558 epoch 2 - iter 2618/3747 - loss 7.04849523 - samples/sec: 14.14 - decode_sents/sec: 31123.94
2022-04-04 04:54:22,328 epoch 2 - iter 2992/3747 - loss 7.02342424 - samples/sec: 15.06 - decode_sents/sec: 50046.49
2022-04-04 04:56:08,669 epoch 2 - iter 3366/3747 - loss 6.98967381 - samples/sec: 14.81 - decode_sents/sec: 131715.83
2022-04-04 04:57:54,543 epoch 2 - iter 3740/3747 - loss 6.95130455 - samples/sec: 14.89 - decode_sents/sec: 33980.37
2022-04-04 04:57:55,777 ----------------------------------------------------------------------------------------------------
2022-04-04 04:57:55,777 EPOCH 2 done: loss 1.7384 - lr 0.045000000000000005
2022-04-04 04:57:55,777 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 04:59:31,643 Macro Average: 95.77	Macro avg loss: 0.43
ColumnCorpus-CONLL03FULL	95.77	
2022-04-04 04:59:31,787 ----------------------------------------------------------------------------------------------------
2022-04-04 04:59:31,787 BAD EPOCHS (no improvement): 11
2022-04-04 04:59:31,787 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 04:59:31,788 ==================Saving the current best model: 95.77==================
2022-04-04 04:59:39,436 ----------------------------------------------------------------------------------------------------
2022-04-04 04:59:39,448 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 04:59:39,727 epoch 3 - iter 0/3747 - loss 6.75057983 - samples/sec: 14.34 - decode_sents/sec: 133.27
2022-04-04 05:01:25,943 epoch 3 - iter 374/3747 - loss 6.12099562 - samples/sec: 14.75 - decode_sents/sec: 48381.01
2022-04-04 05:03:00,474 epoch 3 - iter 748/3747 - loss 6.09540961 - samples/sec: 16.63 - decode_sents/sec: 111105.91
2022-04-04 05:04:37,665 epoch 3 - iter 1122/3747 - loss 6.09119825 - samples/sec: 16.17 - decode_sents/sec: 46450.20
2022-04-04 05:06:15,577 epoch 3 - iter 1496/3747 - loss 6.10168232 - samples/sec: 16.05 - decode_sents/sec: 86711.15
2022-04-04 05:07:52,015 epoch 3 - iter 1870/3747 - loss 6.07669920 - samples/sec: 16.30 - decode_sents/sec: 39400.69
2022-04-04 05:09:25,918 epoch 3 - iter 2244/3747 - loss 6.01189099 - samples/sec: 16.77 - decode_sents/sec: 87880.66
2022-04-04 05:10:58,087 epoch 3 - iter 2618/3747 - loss 6.03175896 - samples/sec: 17.01 - decode_sents/sec: 44544.87
2022-04-04 05:12:29,509 epoch 3 - iter 2992/3747 - loss 6.00727241 - samples/sec: 17.23 - decode_sents/sec: 109141.93
2022-04-04 05:14:17,784 epoch 3 - iter 3366/3747 - loss 6.01102654 - samples/sec: 14.45 - decode_sents/sec: 40987.93
2022-04-04 05:15:49,958 epoch 3 - iter 3740/3747 - loss 6.00828577 - samples/sec: 17.00 - decode_sents/sec: 39417.28
2022-04-04 05:15:51,019 ----------------------------------------------------------------------------------------------------
2022-04-04 05:15:51,019 EPOCH 3 done: loss 1.5014 - lr 0.04000000000000001
2022-04-04 05:15:51,019 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 05:17:13,352 Macro Average: 96.12	Macro avg loss: 0.42
ColumnCorpus-CONLL03FULL	96.12	
2022-04-04 05:17:13,574 ----------------------------------------------------------------------------------------------------
2022-04-04 05:17:13,575 BAD EPOCHS (no improvement): 11
2022-04-04 05:17:13,575 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 05:17:13,575 ==================Saving the current best model: 96.12==================
2022-04-04 05:17:21,150 ----------------------------------------------------------------------------------------------------
2022-04-04 05:17:21,161 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 05:17:21,433 epoch 4 - iter 0/3747 - loss 7.71595764 - samples/sec: 14.75 - decode_sents/sec: 145.56
2022-04-04 05:18:47,604 epoch 4 - iter 374/3747 - loss 5.38498480 - samples/sec: 18.22 - decode_sents/sec: 89249.40
2022-04-04 05:20:24,542 epoch 4 - iter 748/3747 - loss 5.37571929 - samples/sec: 16.12 - decode_sents/sec: 71098.76
2022-04-04 05:21:55,401 epoch 4 - iter 1122/3747 - loss 5.51243585 - samples/sec: 17.25 - decode_sents/sec: 42869.78
2022-04-04 05:23:22,593 epoch 4 - iter 1496/3747 - loss 5.42888041 - samples/sec: 18.01 - decode_sents/sec: 86778.30
2022-04-04 05:24:48,862 epoch 4 - iter 1870/3747 - loss 5.42747452 - samples/sec: 18.21 - decode_sents/sec: 76561.55
2022-04-04 05:26:29,129 epoch 4 - iter 2244/3747 - loss 5.47451539 - samples/sec: 15.64 - decode_sents/sec: 33436.60
2022-04-04 05:28:14,427 epoch 4 - iter 2618/3747 - loss 5.48663043 - samples/sec: 14.89 - decode_sents/sec: 49392.56
2022-04-04 05:30:07,618 epoch 4 - iter 2992/3747 - loss 5.50067425 - samples/sec: 13.82 - decode_sents/sec: 319403.35
2022-04-04 05:31:52,682 epoch 4 - iter 3366/3747 - loss 5.49099762 - samples/sec: 14.93 - decode_sents/sec: 53776.82
2022-04-04 05:33:49,263 epoch 4 - iter 3740/3747 - loss 5.48234194 - samples/sec: 13.40 - decode_sents/sec: 58236.92
2022-04-04 05:33:50,466 ----------------------------------------------------------------------------------------------------
2022-04-04 05:33:50,466 EPOCH 4 done: loss 1.3702 - lr 0.034999999999999996
2022-04-04 05:33:50,466 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 05:35:12,546 Macro Average: 96.45	Macro avg loss: 0.40
ColumnCorpus-CONLL03FULL	96.45	
2022-04-04 05:35:12,732 ----------------------------------------------------------------------------------------------------
2022-04-04 05:35:12,733 BAD EPOCHS (no improvement): 11
2022-04-04 05:35:12,733 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 05:35:12,733 ==================Saving the current best model: 96.45==================
2022-04-04 05:35:20,297 ----------------------------------------------------------------------------------------------------
2022-04-04 05:35:20,309 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 05:35:20,783 epoch 5 - iter 0/3747 - loss 11.97894287 - samples/sec: 8.44 - decode_sents/sec: 98.86
2022-04-04 05:37:02,607 epoch 5 - iter 374/3747 - loss 5.28396531 - samples/sec: 15.41 - decode_sents/sec: 99574.37
2022-04-04 05:38:38,882 epoch 5 - iter 748/3747 - loss 5.09680364 - samples/sec: 16.32 - decode_sents/sec: 36467.55
2022-04-04 05:40:14,542 epoch 5 - iter 1122/3747 - loss 5.11469792 - samples/sec: 16.44 - decode_sents/sec: 42638.77
2022-04-04 05:41:52,186 epoch 5 - iter 1496/3747 - loss 5.05037952 - samples/sec: 16.02 - decode_sents/sec: 52110.08
2022-04-04 05:43:28,236 epoch 5 - iter 1870/3747 - loss 5.08718409 - samples/sec: 16.34 - decode_sents/sec: 36442.96
2022-04-04 05:45:14,767 epoch 5 - iter 2244/3747 - loss 5.12045963 - samples/sec: 14.78 - decode_sents/sec: 51328.72
2022-04-04 05:47:00,133 epoch 5 - iter 2618/3747 - loss 5.11098262 - samples/sec: 14.96 - decode_sents/sec: 31818.05
2022-04-04 05:48:42,850 epoch 5 - iter 2992/3747 - loss 5.13074100 - samples/sec: 15.37 - decode_sents/sec: 64720.10
2022-04-04 05:50:23,086 epoch 5 - iter 3366/3747 - loss 5.12152803 - samples/sec: 15.69 - decode_sents/sec: 82241.25
2022-04-04 05:51:59,678 epoch 5 - iter 3740/3747 - loss 5.08745078 - samples/sec: 16.24 - decode_sents/sec: 41491.51
2022-04-04 05:52:01,038 ----------------------------------------------------------------------------------------------------
2022-04-04 05:52:01,038 EPOCH 5 done: loss 1.2723 - lr 0.03
2022-04-04 05:52:01,038 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 05:53:31,203 Macro Average: 96.48	Macro avg loss: 0.43
ColumnCorpus-CONLL03FULL	96.48	
2022-04-04 05:53:31,379 ----------------------------------------------------------------------------------------------------
2022-04-04 05:53:31,379 BAD EPOCHS (no improvement): 11
2022-04-04 05:53:31,379 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 05:53:31,379 ==================Saving the current best model: 96.48==================
2022-04-04 05:53:39,103 ----------------------------------------------------------------------------------------------------
2022-04-04 05:53:39,112 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 05:53:39,442 epoch 6 - iter 0/3747 - loss 6.16476440 - samples/sec: 12.13 - decode_sents/sec: 128.48
2022-04-04 05:55:29,533 epoch 6 - iter 374/3747 - loss 5.01230019 - samples/sec: 14.21 - decode_sents/sec: 403386.61
2022-04-04 05:57:14,738 epoch 6 - iter 748/3747 - loss 4.87342831 - samples/sec: 14.92 - decode_sents/sec: 50012.19
2022-04-04 05:58:53,263 epoch 6 - iter 1122/3747 - loss 4.79845175 - samples/sec: 15.92 - decode_sents/sec: 59815.81
2022-04-04 06:00:25,125 epoch 6 - iter 1496/3747 - loss 4.77599582 - samples/sec: 17.12 - decode_sents/sec: 58771.48
2022-04-04 06:02:10,438 epoch 6 - iter 1870/3747 - loss 4.81279716 - samples/sec: 14.84 - decode_sents/sec: 41258.07
2022-04-04 06:03:43,519 epoch 6 - iter 2244/3747 - loss 4.81402353 - samples/sec: 16.93 - decode_sents/sec: 80165.05
2022-04-04 06:05:21,562 epoch 6 - iter 2618/3747 - loss 4.82418321 - samples/sec: 16.03 - decode_sents/sec: 48867.83
2022-04-04 06:06:58,157 epoch 6 - iter 2992/3747 - loss 4.83971538 - samples/sec: 16.29 - decode_sents/sec: 50835.52
2022-04-04 06:08:34,891 epoch 6 - iter 3366/3747 - loss 4.81167900 - samples/sec: 16.26 - decode_sents/sec: 225068.29
2022-04-04 06:10:15,494 epoch 6 - iter 3740/3747 - loss 4.81217857 - samples/sec: 15.62 - decode_sents/sec: 64750.83
2022-04-04 06:10:17,019 ----------------------------------------------------------------------------------------------------
2022-04-04 06:10:17,019 EPOCH 6 done: loss 1.2032 - lr 0.025
2022-04-04 06:10:17,019 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 06:11:47,155 Macro Average: 96.49	Macro avg loss: 0.43
ColumnCorpus-CONLL03FULL	96.49	
2022-04-04 06:11:47,400 ----------------------------------------------------------------------------------------------------
2022-04-04 06:11:47,400 BAD EPOCHS (no improvement): 11
2022-04-04 06:11:47,400 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 06:11:47,400 ==================Saving the current best model: 96.49==================
2022-04-04 06:11:55,063 ----------------------------------------------------------------------------------------------------
2022-04-04 06:11:55,074 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 06:11:55,200 epoch 7 - iter 0/3747 - loss 0.59625435 - samples/sec: 31.98 - decode_sents/sec: 278.64
2022-04-04 06:13:41,268 epoch 7 - iter 374/3747 - loss 4.42937026 - samples/sec: 14.82 - decode_sents/sec: 139841.31
2022-04-04 06:15:16,532 epoch 7 - iter 748/3747 - loss 4.48704898 - samples/sec: 16.52 - decode_sents/sec: 35285.30
2022-04-04 06:16:51,688 epoch 7 - iter 1122/3747 - loss 4.58959888 - samples/sec: 16.54 - decode_sents/sec: 39123.58
2022-04-04 06:18:16,893 epoch 7 - iter 1496/3747 - loss 4.52232426 - samples/sec: 18.45 - decode_sents/sec: 49748.89
2022-04-04 06:19:47,617 epoch 7 - iter 1870/3747 - loss 4.51041191 - samples/sec: 17.27 - decode_sents/sec: 120174.65
2022-04-04 06:21:16,445 epoch 7 - iter 2244/3747 - loss 4.56923931 - samples/sec: 17.66 - decode_sents/sec: 110327.90
2022-04-04 06:22:52,980 epoch 7 - iter 2618/3747 - loss 4.58884662 - samples/sec: 16.19 - decode_sents/sec: 40988.20
2022-04-04 06:24:36,375 epoch 7 - iter 2992/3747 - loss 4.56011075 - samples/sec: 15.25 - decode_sents/sec: 24323.29
2022-04-04 06:26:20,480 epoch 7 - iter 3366/3747 - loss 4.56872770 - samples/sec: 15.16 - decode_sents/sec: 49985.09
2022-04-04 06:28:08,976 epoch 7 - iter 3740/3747 - loss 4.58139329 - samples/sec: 14.50 - decode_sents/sec: 68326.31
2022-04-04 06:28:10,439 ----------------------------------------------------------------------------------------------------
2022-04-04 06:28:10,440 EPOCH 7 done: loss 1.1458 - lr 0.020000000000000004
2022-04-04 06:28:10,440 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 06:29:54,587 Macro Average: 96.45	Macro avg loss: 0.45
ColumnCorpus-CONLL03FULL	96.45	
2022-04-04 06:29:54,745 ----------------------------------------------------------------------------------------------------
2022-04-04 06:29:54,745 BAD EPOCHS (no improvement): 11
2022-04-04 06:29:54,745 GLOBAL BAD EPOCHS (no improvement): 1
2022-04-04 06:29:54,745 ----------------------------------------------------------------------------------------------------
2022-04-04 06:29:54,748 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 06:29:55,037 epoch 8 - iter 0/3747 - loss 1.99517822 - samples/sec: 13.85 - decode_sents/sec: 119.88
2022-04-04 06:31:38,871 epoch 8 - iter 374/3747 - loss 4.46845509 - samples/sec: 15.20 - decode_sents/sec: 352985.98
2022-04-04 06:33:23,701 epoch 8 - iter 748/3747 - loss 4.42484427 - samples/sec: 15.05 - decode_sents/sec: 29377.76
2022-04-04 06:35:08,127 epoch 8 - iter 1122/3747 - loss 4.34962994 - samples/sec: 15.10 - decode_sents/sec: 31109.43
2022-04-04 06:36:52,472 epoch 8 - iter 1496/3747 - loss 4.42048952 - samples/sec: 15.14 - decode_sents/sec: 37231.16
2022-04-04 06:38:37,994 epoch 8 - iter 1870/3747 - loss 4.42501553 - samples/sec: 14.93 - decode_sents/sec: 51002.04
2022-04-04 06:40:20,779 epoch 8 - iter 2244/3747 - loss 4.42707979 - samples/sec: 15.36 - decode_sents/sec: 340750.16
2022-04-04 06:42:04,929 epoch 8 - iter 2618/3747 - loss 4.40680538 - samples/sec: 15.15 - decode_sents/sec: 90899.17
2022-04-04 06:43:59,174 epoch 8 - iter 2992/3747 - loss 4.43276420 - samples/sec: 13.75 - decode_sents/sec: 27033.91
2022-04-04 06:45:44,052 epoch 8 - iter 3366/3747 - loss 4.42384261 - samples/sec: 15.04 - decode_sents/sec: 70100.31
2022-04-04 06:47:31,495 epoch 8 - iter 3740/3747 - loss 4.45615100 - samples/sec: 14.65 - decode_sents/sec: 178384.61
2022-04-04 06:47:33,324 ----------------------------------------------------------------------------------------------------
2022-04-04 06:47:33,324 EPOCH 8 done: loss 1.1156 - lr 0.015
2022-04-04 06:47:33,324 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 06:49:09,287 Macro Average: 96.51	Macro avg loss: 0.44
ColumnCorpus-CONLL03FULL	96.51	
2022-04-04 06:49:09,509 ----------------------------------------------------------------------------------------------------
2022-04-04 06:49:09,509 BAD EPOCHS (no improvement): 11
2022-04-04 06:49:09,509 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 06:49:09,509 ==================Saving the current best model: 96.50999999999999==================
2022-04-04 06:49:17,275 ----------------------------------------------------------------------------------------------------
2022-04-04 06:49:17,287 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 06:49:17,768 epoch 9 - iter 0/3747 - loss 4.42678833 - samples/sec: 8.32 - decode_sents/sec: 95.94
2022-04-04 06:51:03,442 epoch 9 - iter 374/3747 - loss 4.39917736 - samples/sec: 14.85 - decode_sents/sec: 46353.43
2022-04-04 06:53:03,005 epoch 9 - iter 748/3747 - loss 4.49290728 - samples/sec: 13.05 - decode_sents/sec: 50205.86
2022-04-04 06:54:48,345 epoch 9 - iter 1122/3747 - loss 4.48121946 - samples/sec: 14.90 - decode_sents/sec: 146194.75
2022-04-04 06:56:20,929 epoch 9 - iter 1496/3747 - loss 4.44771790 - samples/sec: 16.94 - decode_sents/sec: 49934.97
2022-04-04 06:57:51,067 epoch 9 - iter 1870/3747 - loss 4.44293176 - samples/sec: 17.38 - decode_sents/sec: 78232.04
2022-04-04 06:59:24,569 epoch 9 - iter 2244/3747 - loss 4.44623976 - samples/sec: 16.74 - decode_sents/sec: 45533.03
2022-04-04 07:00:54,454 epoch 9 - iter 2618/3747 - loss 4.42971311 - samples/sec: 17.47 - decode_sents/sec: 63241.34
2022-04-04 07:02:32,119 epoch 9 - iter 2992/3747 - loss 4.40830804 - samples/sec: 16.12 - decode_sents/sec: 47775.77
2022-04-04 07:03:59,243 epoch 9 - iter 3366/3747 - loss 4.38848372 - samples/sec: 18.01 - decode_sents/sec: 72171.69
2022-04-04 07:05:34,950 epoch 9 - iter 3740/3747 - loss 4.38072436 - samples/sec: 16.34 - decode_sents/sec: 49718.15
2022-04-04 07:05:36,135 ----------------------------------------------------------------------------------------------------
2022-04-04 07:05:36,136 EPOCH 9 done: loss 1.0949 - lr 0.010000000000000002
2022-04-04 07:05:36,136 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 07:06:58,370 Macro Average: 96.52	Macro avg loss: 0.45
ColumnCorpus-CONLL03FULL	96.52	
2022-04-04 07:06:58,622 ----------------------------------------------------------------------------------------------------
2022-04-04 07:06:58,622 BAD EPOCHS (no improvement): 11
2022-04-04 07:06:58,622 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-04 07:06:58,622 ==================Saving the current best model: 96.52==================
2022-04-04 07:07:06,177 ----------------------------------------------------------------------------------------------------
2022-04-04 07:07:06,188 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-04 07:07:06,325 epoch 10 - iter 0/3747 - loss 0.28293228 - samples/sec: 29.21 - decode_sents/sec: 276.45
2022-04-04 07:08:49,919 epoch 10 - iter 374/3747 - loss 4.42062114 - samples/sec: 15.23 - decode_sents/sec: 63010.17
2022-04-04 07:10:35,867 epoch 10 - iter 748/3747 - loss 4.43628141 - samples/sec: 14.88 - decode_sents/sec: 64570.25
2022-04-04 07:12:28,301 epoch 10 - iter 1122/3747 - loss 4.40175010 - samples/sec: 13.97 - decode_sents/sec: 40789.96
2022-04-04 07:14:10,304 epoch 10 - iter 1496/3747 - loss 4.32699082 - samples/sec: 15.50 - decode_sents/sec: 32634.39
2022-04-04 07:15:55,958 epoch 10 - iter 1870/3747 - loss 4.35295895 - samples/sec: 14.92 - decode_sents/sec: 34849.45
2022-04-04 07:17:41,038 epoch 10 - iter 2244/3747 - loss 4.34589568 - samples/sec: 14.96 - decode_sents/sec: 34245.85
2022-04-04 07:19:25,481 epoch 10 - iter 2618/3747 - loss 4.32350994 - samples/sec: 15.01 - decode_sents/sec: 40354.49
2022-04-04 07:21:11,554 epoch 10 - iter 2992/3747 - loss 4.32809183 - samples/sec: 14.79 - decode_sents/sec: 86222.62
2022-04-04 07:22:49,171 epoch 10 - iter 3366/3747 - loss 4.32630630 - samples/sec: 16.09 - decode_sents/sec: 95765.92
2022-04-04 07:24:20,480 epoch 10 - iter 3740/3747 - loss 4.31295228 - samples/sec: 17.18 - decode_sents/sec: 40937.93
2022-04-04 07:24:22,117 ----------------------------------------------------------------------------------------------------
2022-04-04 07:24:22,117 EPOCH 10 done: loss 1.0779 - lr 0.005000000000000001
2022-04-04 07:24:22,117 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-04 07:25:52,330 Macro Average: 96.52	Macro avg loss: 0.45
ColumnCorpus-CONLL03FULL	96.52	
2022-04-04 07:25:52,530 ----------------------------------------------------------------------------------------------------
2022-04-04 07:25:52,530 BAD EPOCHS (no improvement): 11
2022-04-04 07:25:52,530 GLOBAL BAD EPOCHS (no improvement): 1
2022-04-04 07:25:52,530 ==================Saving the current best model: 96.52==================
2022-04-04 07:26:00,295 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2107 train
2022-04-04 07:26:00,300 loading file resources/taggers/ln_augment_data_conll03_epoch104/best-model.pt
[2022-04-04 07:26:03,805 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-04 07:26:03,806 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-04 07:26:04,810 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
2022-04-04 07:26:05,320 Testing using best model ...
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2138 train <torch.utils.data.dataset.ConcatDataset object at 0x7fb9d0b652e8>
2022-04-04 07:26:05,892 xlm-roberta-large 559890432
2022-04-04 07:26:05,892 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-04 07:27:15,352 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/ln_augment_data_conll03_epoch104/test.tsv
2022-04-04 07:27:30,962 0.9279	0.9386	0.9332
2022-04-04 07:27:30,962 
MICRO_AVG: acc 0.8748 - f1-score 0.9332
MACRO_AVG: acc 0.8551 - f1-score 0.9194
LOC        tp: 1575 - fp: 91 - fn: 93 - tn: 1575 - precision: 0.9454 - recall: 0.9442 - accuracy: 0.8954 - f1-score: 0.9448
MISC       tp: 599 - fp: 136 - fn: 103 - tn: 599 - precision: 0.8150 - recall: 0.8533 - accuracy: 0.7148 - f1-score: 0.8337
ORG        tp: 1549 - fp: 148 - fn: 112 - tn: 1549 - precision: 0.9128 - recall: 0.9326 - accuracy: 0.8563 - f1-score: 0.9226
PER        tp: 1578 - fp: 37 - fn: 39 - tn: 1578 - precision: 0.9771 - recall: 0.9759 - accuracy: 0.9541 - f1-score: 0.9765
2022-04-04 07:27:30,962 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2226 train
2022-04-04 07:27:30,962 ----------------------------------------------------------------------------------------------------
2022-04-04 07:27:30,962 current corpus: ColumnCorpus-CONLL03FULL
2022-04-04 07:27:31,123 xlm-roberta-large 559890432
2022-04-04 07:27:31,123 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-04 07:27:33,315 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/ln_augment_data_conll03_epoch104/ColumnCorpus-CONLL03FULL-test.tsv
2022-04-04 07:27:48,750 0.9279	0.9386	0.9332
2022-04-04 07:27:48,751 
MICRO_AVG: acc 0.8748 - f1-score 0.9332
MACRO_AVG: acc 0.8551 - f1-score 0.9194
LOC        tp: 1575 - fp: 91 - fn: 93 - tn: 1575 - precision: 0.9454 - recall: 0.9442 - accuracy: 0.8954 - f1-score: 0.9448
MISC       tp: 599 - fp: 136 - fn: 103 - tn: 599 - precision: 0.8150 - recall: 0.8533 - accuracy: 0.7148 - f1-score: 0.8337
ORG        tp: 1549 - fp: 148 - fn: 112 - tn: 1549 - precision: 0.9128 - recall: 0.9326 - accuracy: 0.8563 - f1-score: 0.9226
PER        tp: 1578 - fp: 37 - fn: 39 - tn: 1578 - precision: 0.9771 - recall: 0.9759 - accuracy: 0.9541 - f1-score: 0.9765

