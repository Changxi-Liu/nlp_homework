/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/miao/anaconda3/envs/nlp-3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2022-04-01 18:50:56,352 Reading data from /home/miao/6207/lcx/CLNER/CLNER_datasets/wnut17_bertscore_eos_doc_full2
2022-04-01 18:50:56,352 Train: /home/miao/6207/lcx/CLNER/CLNER_datasets/wnut17_bertscore_eos_doc_full2/train.txt
2022-04-01 18:50:56,352 Dev: /home/miao/6207/lcx/CLNER/CLNER_datasets/wnut17_bertscore_eos_doc_full2/dev.txt
2022-04-01 18:50:56,352 Test: /home/miao/6207/lcx/CLNER/CLNER_datasets/wnut17_bertscore_eos_doc_full2/test.txt
2022-04-01 18:51:05,076 {b'<unk>': 0, b'O': 1, b'B-location': 2, b'I-location': 3, b'E-location': 4, b'S-location': 5, b'S-X': 6, b'S-group': 7, b'S-corporation': 8, b'S-person': 9, b'S-creative-work': 10, b'S-product': 11, b'B-person': 12, b'E-person': 13, b'B-creative-work': 14, b'I-creative-work': 15, b'E-creative-work': 16, b'B-corporation': 17, b'I-corporation': 18, b'E-corporation': 19, b'B-group': 20, b'I-group': 21, b'E-group': 22, b'I-person': 23, b'B-product': 24, b'I-product': 25, b'E-product': 26, b'<START>': 27, b'<STOP>': 28}
2022-04-01 18:51:05,076 Corpus: 6788 train + 1009 dev + 1287 test sentences
/home/miao/6207/lcx/CLNER/flair/utils/params.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  dict_merge.dict_merge(params_dict, yaml.load(f))
[2022-04-01 18:51:06,351 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-01 18:51:06,352 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-01 18:51:07,352 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
[2022-04-01 18:51:08,812 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-01 18:51:08,813 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-01 18:51:08,971 INFO] loading weights file https://cdn.huggingface.co/xlm-roberta-large-pytorch_model.bin from cache at /home/miao/.cache/torch/transformers/a89d1c4637c1ea5ecd460c2a7c06a03acc9a961fc8c59aa2dd76d8a7f1e94536.2f41fe28a80f2730715b795242a01fc3dda846a85e7903adb3907dc5c5a498bf
[2022-04-01 18:51:22,800 INFO] All model checkpoint weights were used when initializing XLMRobertaModel.

[2022-04-01 18:51:22,800 INFO] All the weights of XLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use XLMRobertaModel for predictions without further training.
2022-04-01 18:51:26,148 Model Size: 559920998
Corpus: 6788 train + 1009 dev + 1287 test sentences
2022-04-01 18:51:26,168 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,170 Model: "FastSequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): TransformerWordEmbeddings(
      (model): XLMRobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(250002, 1024, padding_idx=1)
          (position_embeddings): Embedding(514, 1024, padding_idx=1)
          (token_type_embeddings): Embedding(1, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (12): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (13): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (14): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (15): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (16): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (17): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (18): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (19): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (20): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (21): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (22): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (23): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.1)
  (linear): Linear(in_features=1024, out_features=29, bias=True)
)"
2022-04-01 18:51:26,171 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,171 Corpus: "Corpus: 6788 train + 1009 dev + 1287 test sentences"
2022-04-01 18:51:26,171 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,171 Parameters:
2022-04-01 18:51:26,171  - Optimizer: "AdamW"
2022-04-01 18:51:26,171  - learning_rate: "5e-06"
2022-04-01 18:51:26,171  - mini_batch_size: "2"
2022-04-01 18:51:26,171  - patience: "10"
2022-04-01 18:51:26,171  - anneal_factor: "0.5"
2022-04-01 18:51:26,171  - max_epochs: "20"
2022-04-01 18:51:26,171  - shuffle: "True"
2022-04-01 18:51:26,171  - train_with_dev: "False"
2022-04-01 18:51:26,171  - word min_freq: "-1"
2022-04-01 18:51:26,171 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,171 Model training base path: "resources/taggers/xlmr-first_10epoch_2batch_2accumulate_0.000005lr_10000lrrate_eng_monolingual_crf_fast_norelearn_sentbatch_sentloss_finetune_nodev_wnut_doc_full_bertscore_eos_ner9"
2022-04-01 18:51:26,171 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,171 Device: cuda:0
2022-04-01 18:51:26,171 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:26,171 Embeddings storage mode: none
2022-04-01 18:51:27,235 ----------------------------------------------------------------------------------------------------
2022-04-01 18:51:27,238 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 18:51:28,166 epoch 1 - iter 0/3394 - loss 525.08605957 - samples/sec: 2.16 - decode_sents/sec: 62.83
2022-04-01 18:52:29,827 epoch 1 - iter 339/3394 - loss 36.88058587 - samples/sec: 12.71 - decode_sents/sec: 91300.55
2022-04-01 18:53:28,791 epoch 1 - iter 678/3394 - loss 23.69149267 - samples/sec: 13.45 - decode_sents/sec: 24513.72
2022-04-01 18:54:23,439 epoch 1 - iter 1017/3394 - loss 18.38906021 - samples/sec: 14.57 - decode_sents/sec: 17223.19
2022-04-01 18:55:14,136 epoch 1 - iter 1356/3394 - loss 15.56226490 - samples/sec: 15.64 - decode_sents/sec: 53006.36
2022-04-01 18:56:06,037 epoch 1 - iter 1695/3394 - loss 13.72255030 - samples/sec: 15.33 - decode_sents/sec: 37327.73
2022-04-01 18:56:59,369 epoch 1 - iter 2034/3394 - loss 12.46659057 - samples/sec: 14.85 - decode_sents/sec: 23808.13
2022-04-01 18:57:48,755 epoch 1 - iter 2373/3394 - loss 11.56891967 - samples/sec: 15.99 - decode_sents/sec: 17474.33
2022-04-01 18:58:41,917 epoch 1 - iter 2712/3394 - loss 10.81956608 - samples/sec: 14.88 - decode_sents/sec: 157190.76
2022-04-01 18:59:39,092 epoch 1 - iter 3051/3394 - loss 10.24977091 - samples/sec: 13.87 - decode_sents/sec: 31328.02
2022-04-01 19:00:29,693 epoch 1 - iter 3390/3394 - loss 9.69122649 - samples/sec: 15.77 - decode_sents/sec: 109885.94
2022-04-01 19:00:30,164 ----------------------------------------------------------------------------------------------------
2022-04-01 19:00:30,164 EPOCH 1 done: loss 4.8415 - lr 0.05
2022-04-01 19:00:30,164 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:01:02,291 Macro Average: 69.13	Macro avg loss: 2.12
ColumnCorpus-WNUTDOCFULL	69.13	
2022-04-01 19:01:02,363 ----------------------------------------------------------------------------------------------------
2022-04-01 19:01:02,363 BAD EPOCHS (no improvement): 11
2022-04-01 19:01:02,363 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-01 19:01:02,363 ==================Saving the current best model: 69.13==================
2022-04-01 19:01:10,541 ----------------------------------------------------------------------------------------------------
2022-04-01 19:01:10,549 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:01:10,645 epoch 2 - iter 0/3394 - loss 4.57670593 - samples/sec: 20.79 - decode_sents/sec: 175.22
2022-04-01 19:02:03,317 epoch 2 - iter 339/3394 - loss 4.47903137 - samples/sec: 15.02 - decode_sents/sec: 26959.72
2022-04-01 19:03:03,399 epoch 2 - iter 678/3394 - loss 4.67506768 - samples/sec: 13.04 - decode_sents/sec: 20566.26
2022-04-01 19:03:59,638 epoch 2 - iter 1017/3394 - loss 4.53688238 - samples/sec: 14.06 - decode_sents/sec: 105183.39
2022-04-01 19:04:53,694 epoch 2 - iter 1356/3394 - loss 4.51550050 - samples/sec: 14.68 - decode_sents/sec: 33605.19
2022-04-01 19:05:42,308 epoch 2 - iter 1695/3394 - loss 4.42329031 - samples/sec: 16.39 - decode_sents/sec: 158275.62
2022-04-01 19:06:32,769 epoch 2 - iter 2034/3394 - loss 4.48725625 - samples/sec: 15.57 - decode_sents/sec: 17523.65
2022-04-01 19:07:20,702 epoch 2 - iter 2373/3394 - loss 4.46121840 - samples/sec: 16.65 - decode_sents/sec: 83487.12
2022-04-01 19:08:15,827 epoch 2 - iter 2712/3394 - loss 4.45163748 - samples/sec: 14.44 - decode_sents/sec: 26480.97
2022-04-01 19:09:08,870 epoch 2 - iter 3051/3394 - loss 4.41009910 - samples/sec: 14.93 - decode_sents/sec: 20791.21
2022-04-01 19:10:01,826 epoch 2 - iter 3390/3394 - loss 4.31700443 - samples/sec: 15.14 - decode_sents/sec: 50795.55
2022-04-01 19:10:02,510 ----------------------------------------------------------------------------------------------------
2022-04-01 19:10:02,510 EPOCH 2 done: loss 2.1591 - lr 0.0475
2022-04-01 19:10:02,510 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:10:34,997 Macro Average: 69.29	Macro avg loss: 2.28
ColumnCorpus-WNUTDOCFULL	69.29	
2022-04-01 19:10:35,059 ----------------------------------------------------------------------------------------------------
2022-04-01 19:10:35,059 BAD EPOCHS (no improvement): 11
2022-04-01 19:10:35,059 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-01 19:10:35,059 ==================Saving the current best model: 69.28999999999999==================
2022-04-01 19:10:43,208 ----------------------------------------------------------------------------------------------------
2022-04-01 19:10:43,220 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:10:43,408 epoch 3 - iter 0/3394 - loss 4.13369751 - samples/sec: 10.62 - decode_sents/sec: 90.73
2022-04-01 19:11:35,454 epoch 3 - iter 339/3394 - loss 3.18856238 - samples/sec: 15.21 - decode_sents/sec: 142800.95
2022-04-01 19:12:27,452 epoch 3 - iter 678/3394 - loss 3.39163588 - samples/sec: 15.26 - decode_sents/sec: 195634.16
2022-04-01 19:13:15,478 epoch 3 - iter 1017/3394 - loss 3.40064477 - samples/sec: 16.63 - decode_sents/sec: 16975.62
2022-04-01 19:14:06,559 epoch 3 - iter 1356/3394 - loss 3.40213458 - samples/sec: 15.35 - decode_sents/sec: 22115.11
2022-04-01 19:14:53,884 epoch 3 - iter 1695/3394 - loss 3.39636578 - samples/sec: 16.78 - decode_sents/sec: 111615.44
2022-04-01 19:15:41,825 epoch 3 - iter 2034/3394 - loss 3.41031575 - samples/sec: 16.53 - decode_sents/sec: 46241.90
2022-04-01 19:16:31,234 epoch 3 - iter 2373/3394 - loss 3.41519704 - samples/sec: 15.96 - decode_sents/sec: 149230.59
2022-04-01 19:17:16,943 epoch 3 - iter 2712/3394 - loss 3.39804718 - samples/sec: 17.48 - decode_sents/sec: 22123.20
2022-04-01 19:18:09,465 epoch 3 - iter 3051/3394 - loss 3.42871186 - samples/sec: 15.08 - decode_sents/sec: 19365.18
2022-04-01 19:19:05,810 epoch 3 - iter 3390/3394 - loss 3.43235359 - samples/sec: 14.06 - decode_sents/sec: 19887.95
2022-04-01 19:19:06,274 ----------------------------------------------------------------------------------------------------
2022-04-01 19:19:06,274 EPOCH 3 done: loss 1.7153 - lr 0.045000000000000005
2022-04-01 19:19:06,274 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:19:38,785 Macro Average: 69.23	Macro avg loss: 2.72
ColumnCorpus-WNUTDOCFULL	69.23	
2022-04-01 19:19:38,852 ----------------------------------------------------------------------------------------------------
2022-04-01 19:19:38,852 BAD EPOCHS (no improvement): 11
2022-04-01 19:19:38,852 GLOBAL BAD EPOCHS (no improvement): 1
2022-04-01 19:19:38,852 ----------------------------------------------------------------------------------------------------
2022-04-01 19:19:38,855 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:19:39,135 epoch 4 - iter 0/3394 - loss 19.20050049 - samples/sec: 7.16 - decode_sents/sec: 55.74
2022-04-01 19:20:30,579 epoch 4 - iter 339/3394 - loss 2.90473601 - samples/sec: 15.47 - decode_sents/sec: 105628.78
2022-04-01 19:21:29,206 epoch 4 - iter 678/3394 - loss 2.90359328 - samples/sec: 13.54 - decode_sents/sec: 83240.29
2022-04-01 19:22:26,590 epoch 4 - iter 1017/3394 - loss 3.03772670 - samples/sec: 13.74 - decode_sents/sec: 61389.33
2022-04-01 19:23:25,433 epoch 4 - iter 1356/3394 - loss 2.97481264 - samples/sec: 13.44 - decode_sents/sec: 144623.82
2022-04-01 19:24:15,305 epoch 4 - iter 1695/3394 - loss 2.95094554 - samples/sec: 15.79 - decode_sents/sec: 81419.48
2022-04-01 19:25:02,956 epoch 4 - iter 2034/3394 - loss 3.02877738 - samples/sec: 16.67 - decode_sents/sec: 97629.02
2022-04-01 19:25:54,994 epoch 4 - iter 2373/3394 - loss 2.99047351 - samples/sec: 15.26 - decode_sents/sec: 16679.70
2022-04-01 19:26:49,596 epoch 4 - iter 2712/3394 - loss 3.04751277 - samples/sec: 14.48 - decode_sents/sec: 23164.27
2022-04-01 19:27:37,724 epoch 4 - iter 3051/3394 - loss 2.98571423 - samples/sec: 16.70 - decode_sents/sec: 29041.44
2022-04-01 19:28:28,406 epoch 4 - iter 3390/3394 - loss 2.97089282 - samples/sec: 15.49 - decode_sents/sec: 30776.72
2022-04-01 19:28:28,979 ----------------------------------------------------------------------------------------------------
2022-04-01 19:28:28,979 EPOCH 4 done: loss 1.4848 - lr 0.0425
2022-04-01 19:28:28,979 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:28:59,668 Macro Average: 70.72	Macro avg loss: 2.85
ColumnCorpus-WNUTDOCFULL	70.72	
2022-04-01 19:28:59,730 ----------------------------------------------------------------------------------------------------
2022-04-01 19:28:59,730 BAD EPOCHS (no improvement): 11
2022-04-01 19:28:59,730 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-01 19:28:59,730 ==================Saving the current best model: 70.72==================
2022-04-01 19:29:07,908 ----------------------------------------------------------------------------------------------------
2022-04-01 19:29:07,919 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:29:08,024 epoch 5 - iter 0/3394 - loss 0.51507568 - samples/sec: 19.14 - decode_sents/sec: 205.11
2022-04-01 19:29:58,001 epoch 5 - iter 339/3394 - loss 2.53763027 - samples/sec: 15.83 - decode_sents/sec: 18933.89
2022-04-01 19:30:46,028 epoch 5 - iter 678/3394 - loss 2.61842713 - samples/sec: 16.49 - decode_sents/sec: 19806.22
2022-04-01 19:31:41,926 epoch 5 - iter 1017/3394 - loss 2.60525837 - samples/sec: 14.22 - decode_sents/sec: 71788.00
2022-04-01 19:32:33,031 epoch 5 - iter 1356/3394 - loss 2.59229463 - samples/sec: 15.61 - decode_sents/sec: 97218.49
2022-04-01 19:33:20,260 epoch 5 - iter 1695/3394 - loss 2.59232334 - samples/sec: 16.83 - decode_sents/sec: 19871.00
2022-04-01 19:34:15,403 epoch 5 - iter 2034/3394 - loss 2.60521150 - samples/sec: 14.39 - decode_sents/sec: 20386.10
2022-04-01 19:35:12,509 epoch 5 - iter 2373/3394 - loss 2.59429501 - samples/sec: 13.82 - decode_sents/sec: 104418.67
2022-04-01 19:36:08,159 epoch 5 - iter 2712/3394 - loss 2.59013288 - samples/sec: 14.19 - decode_sents/sec: 50531.09
2022-04-01 19:36:56,980 epoch 5 - iter 3051/3394 - loss 2.63964015 - samples/sec: 16.19 - decode_sents/sec: 133464.97
2022-04-01 19:37:43,897 epoch 5 - iter 3390/3394 - loss 2.65872215 - samples/sec: 16.94 - decode_sents/sec: 100308.22
2022-04-01 19:37:44,380 ----------------------------------------------------------------------------------------------------
2022-04-01 19:37:44,380 EPOCH 5 done: loss 1.3306 - lr 0.04000000000000001
2022-04-01 19:37:44,380 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:38:15,001 Macro Average: 69.59	Macro avg loss: 3.14
ColumnCorpus-WNUTDOCFULL	69.59	
2022-04-01 19:38:15,069 ----------------------------------------------------------------------------------------------------
2022-04-01 19:38:15,069 BAD EPOCHS (no improvement): 11
2022-04-01 19:38:15,069 GLOBAL BAD EPOCHS (no improvement): 1
2022-04-01 19:38:15,069 ----------------------------------------------------------------------------------------------------
2022-04-01 19:38:15,072 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:38:15,337 epoch 6 - iter 0/3394 - loss 4.77624512 - samples/sec: 7.56 - decode_sents/sec: 60.90
2022-04-01 19:39:05,056 epoch 6 - iter 339/3394 - loss 2.10981539 - samples/sec: 15.98 - decode_sents/sec: 101046.02
2022-04-01 19:39:55,078 epoch 6 - iter 678/3394 - loss 2.13668906 - samples/sec: 15.92 - decode_sents/sec: 22029.97
2022-04-01 19:40:50,599 epoch 6 - iter 1017/3394 - loss 2.42740985 - samples/sec: 14.18 - decode_sents/sec: 146222.65
2022-04-01 19:41:45,981 epoch 6 - iter 1356/3394 - loss 2.42403158 - samples/sec: 14.39 - decode_sents/sec: 90415.18
2022-04-01 19:42:48,454 epoch 6 - iter 1695/3394 - loss 2.46858154 - samples/sec: 12.63 - decode_sents/sec: 15969.69
2022-04-01 19:43:40,026 epoch 6 - iter 2034/3394 - loss 2.38906296 - samples/sec: 15.37 - decode_sents/sec: 46591.16
2022-04-01 19:44:33,208 epoch 6 - iter 2373/3394 - loss 2.44125034 - samples/sec: 14.89 - decode_sents/sec: 41472.65
2022-04-01 19:45:33,372 epoch 6 - iter 2712/3394 - loss 2.45321934 - samples/sec: 13.04 - decode_sents/sec: 103729.28
2022-04-01 19:46:26,257 epoch 6 - iter 3051/3394 - loss 2.45894455 - samples/sec: 15.02 - decode_sents/sec: 242432.92
2022-04-01 19:47:23,837 epoch 6 - iter 3390/3394 - loss 2.47055850 - samples/sec: 13.88 - decode_sents/sec: 38224.86
2022-04-01 19:47:24,161 ----------------------------------------------------------------------------------------------------
2022-04-01 19:47:24,162 EPOCH 6 done: loss 1.2342 - lr 0.037500000000000006
2022-04-01 19:47:24,162 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:47:54,986 Macro Average: 71.56	Macro avg loss: 3.43
ColumnCorpus-WNUTDOCFULL	71.56	
2022-04-01 19:47:55,050 ----------------------------------------------------------------------------------------------------
2022-04-01 19:47:55,050 BAD EPOCHS (no improvement): 11
2022-04-01 19:47:55,050 GLOBAL BAD EPOCHS (no improvement): 0
2022-04-01 19:47:55,050 ==================Saving the current best model: 71.56==================
2022-04-01 19:48:03,336 ----------------------------------------------------------------------------------------------------
2022-04-01 19:48:03,347 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:48:03,452 epoch 7 - iter 0/3394 - loss 0.11955261 - samples/sec: 19.15 - decode_sents/sec: 192.60
2022-04-01 19:48:52,750 epoch 7 - iter 339/3394 - loss 2.35025189 - samples/sec: 16.16 - decode_sents/sec: 76041.88
2022-04-01 19:49:48,765 epoch 7 - iter 678/3394 - loss 2.39565139 - samples/sec: 13.97 - decode_sents/sec: 30656.61
2022-04-01 19:50:46,232 epoch 7 - iter 1017/3394 - loss 2.29276839 - samples/sec: 13.82 - decode_sents/sec: 86315.13
2022-04-01 19:51:39,329 epoch 7 - iter 1356/3394 - loss 2.35543133 - samples/sec: 14.92 - decode_sents/sec: 21739.29
2022-04-01 19:52:30,773 epoch 7 - iter 1695/3394 - loss 2.32222943 - samples/sec: 15.36 - decode_sents/sec: 59988.15
2022-04-01 19:53:23,223 epoch 7 - iter 2034/3394 - loss 2.24199146 - samples/sec: 15.15 - decode_sents/sec: 116546.64
2022-04-01 19:54:15,471 epoch 7 - iter 2373/3394 - loss 2.31085359 - samples/sec: 15.04 - decode_sents/sec: 21760.58
2022-04-01 19:55:08,795 epoch 7 - iter 2712/3394 - loss 2.27318182 - samples/sec: 15.03 - decode_sents/sec: 26094.61
2022-04-01 19:56:06,183 epoch 7 - iter 3051/3394 - loss 2.28040211 - samples/sec: 13.84 - decode_sents/sec: 78296.75
2022-04-01 19:57:06,305 epoch 7 - iter 3390/3394 - loss 2.28597259 - samples/sec: 13.19 - decode_sents/sec: 107024.13
2022-04-01 19:57:07,021 ----------------------------------------------------------------------------------------------------
2022-04-01 19:57:07,021 EPOCH 7 done: loss 1.1442 - lr 0.034999999999999996
2022-04-01 19:57:07,021 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 19:57:39,651 Macro Average: 71.26	Macro avg loss: 3.96
ColumnCorpus-WNUTDOCFULL	71.26	
2022-04-01 19:57:39,696 ----------------------------------------------------------------------------------------------------
2022-04-01 19:57:39,696 BAD EPOCHS (no improvement): 11
2022-04-01 19:57:39,696 GLOBAL BAD EPOCHS (no improvement): 1
2022-04-01 19:57:39,696 ----------------------------------------------------------------------------------------------------
2022-04-01 19:57:39,699 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 19:57:39,998 epoch 8 - iter 0/3394 - loss 4.22485352 - samples/sec: 6.69 - decode_sents/sec: 46.38
2022-04-01 19:58:38,106 epoch 8 - iter 339/3394 - loss 1.89532990 - samples/sec: 13.59 - decode_sents/sec: 35584.09
2022-04-01 19:59:32,285 epoch 8 - iter 678/3394 - loss 2.20535424 - samples/sec: 14.59 - decode_sents/sec: 24458.69
2022-04-01 20:00:20,632 epoch 8 - iter 1017/3394 - loss 2.24941912 - samples/sec: 16.38 - decode_sents/sec: 144045.09
2022-04-01 20:01:15,000 epoch 8 - iter 1356/3394 - loss 2.17177242 - samples/sec: 14.63 - decode_sents/sec: 103736.84
2022-04-01 20:02:09,682 epoch 8 - iter 1695/3394 - loss 2.15229054 - samples/sec: 14.57 - decode_sents/sec: 136777.36
2022-04-01 20:03:06,170 epoch 8 - iter 2034/3394 - loss 2.12315704 - samples/sec: 14.15 - decode_sents/sec: 93537.86
2022-04-01 20:03:59,806 epoch 8 - iter 2373/3394 - loss 2.09768483 - samples/sec: 14.80 - decode_sents/sec: 35261.11
2022-04-01 20:04:53,110 epoch 8 - iter 2712/3394 - loss 2.10815350 - samples/sec: 14.93 - decode_sents/sec: 19124.00
2022-04-01 20:05:41,864 epoch 8 - iter 3051/3394 - loss 2.10321943 - samples/sec: 16.31 - decode_sents/sec: 14924.31
2022-04-01 20:06:43,999 epoch 8 - iter 3390/3394 - loss 2.10522301 - samples/sec: 12.66 - decode_sents/sec: 65666.15
2022-04-01 20:06:44,533 ----------------------------------------------------------------------------------------------------
2022-04-01 20:06:44,533 EPOCH 8 done: loss 1.0538 - lr 0.0325
2022-04-01 20:06:44,533 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:07:17,091 Macro Average: 70.48	Macro avg loss: 3.90
ColumnCorpus-WNUTDOCFULL	70.48	
2022-04-01 20:07:17,169 ----------------------------------------------------------------------------------------------------
2022-04-01 20:07:17,169 BAD EPOCHS (no improvement): 11
2022-04-01 20:07:17,170 GLOBAL BAD EPOCHS (no improvement): 2
2022-04-01 20:07:17,170 ----------------------------------------------------------------------------------------------------
2022-04-01 20:07:17,174 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:07:17,251 epoch 9 - iter 0/3394 - loss 0.09002686 - samples/sec: 25.94 - decode_sents/sec: 218.96
2022-04-01 20:08:10,252 epoch 9 - iter 339/3394 - loss 2.05022904 - samples/sec: 15.03 - decode_sents/sec: 103153.59
2022-04-01 20:09:03,858 epoch 9 - iter 678/3394 - loss 1.99839095 - samples/sec: 14.88 - decode_sents/sec: 22553.42
2022-04-01 20:09:56,052 epoch 9 - iter 1017/3394 - loss 1.97096246 - samples/sec: 15.25 - decode_sents/sec: 88523.79
2022-04-01 20:10:50,370 epoch 9 - iter 1356/3394 - loss 1.96129689 - samples/sec: 14.64 - decode_sents/sec: 36852.69
2022-04-01 20:11:47,950 epoch 9 - iter 1695/3394 - loss 2.04165676 - samples/sec: 13.80 - decode_sents/sec: 19802.64
2022-04-01 20:12:35,826 epoch 9 - iter 2034/3394 - loss 2.04351045 - samples/sec: 16.58 - decode_sents/sec: 118558.25
2022-04-01 20:13:24,376 epoch 9 - iter 2373/3394 - loss 2.06029032 - samples/sec: 16.30 - decode_sents/sec: 139713.97
2022-04-01 20:14:13,558 epoch 9 - iter 2712/3394 - loss 2.04392730 - samples/sec: 16.18 - decode_sents/sec: 88603.77
2022-04-01 20:15:15,840 epoch 9 - iter 3051/3394 - loss 2.06955678 - samples/sec: 12.55 - decode_sents/sec: 110111.44
2022-04-01 20:16:15,967 epoch 9 - iter 3390/3394 - loss 2.07552745 - samples/sec: 13.06 - decode_sents/sec: 146826.63
2022-04-01 20:16:16,323 ----------------------------------------------------------------------------------------------------
2022-04-01 20:16:16,323 EPOCH 9 done: loss 1.0369 - lr 0.03
2022-04-01 20:16:16,323 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:16:48,968 Macro Average: 69.65	Macro avg loss: 4.46
ColumnCorpus-WNUTDOCFULL	69.65	
2022-04-01 20:16:49,031 ----------------------------------------------------------------------------------------------------
2022-04-01 20:16:49,031 BAD EPOCHS (no improvement): 11
2022-04-01 20:16:49,031 GLOBAL BAD EPOCHS (no improvement): 3
2022-04-01 20:16:49,031 ----------------------------------------------------------------------------------------------------
2022-04-01 20:16:49,034 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:16:49,092 epoch 10 - iter 0/3394 - loss 0.03336716 - samples/sec: 34.65 - decode_sents/sec: 335.99
2022-04-01 20:17:42,204 epoch 10 - iter 339/3394 - loss 1.98593363 - samples/sec: 14.96 - decode_sents/sec: 72511.04
2022-04-01 20:18:34,097 epoch 10 - iter 678/3394 - loss 1.87990731 - samples/sec: 15.32 - decode_sents/sec: 84481.69
2022-04-01 20:19:26,518 epoch 10 - iter 1017/3394 - loss 1.88084492 - samples/sec: 15.16 - decode_sents/sec: 61345.63
2022-04-01 20:20:18,932 epoch 10 - iter 1356/3394 - loss 1.86376414 - samples/sec: 15.18 - decode_sents/sec: 80179.83
2022-04-01 20:21:17,656 epoch 10 - iter 1695/3394 - loss 1.89527036 - samples/sec: 13.35 - decode_sents/sec: 145266.56
2022-04-01 20:22:09,784 epoch 10 - iter 2034/3394 - loss 1.88497236 - samples/sec: 15.21 - decode_sents/sec: 16282.22
2022-04-01 20:23:00,657 epoch 10 - iter 2373/3394 - loss 1.90041630 - samples/sec: 15.68 - decode_sents/sec: 156852.63
2022-04-01 20:23:50,644 epoch 10 - iter 2712/3394 - loss 1.88404631 - samples/sec: 15.81 - decode_sents/sec: 28484.10
2022-04-01 20:24:41,653 epoch 10 - iter 3051/3394 - loss 1.88272289 - samples/sec: 15.54 - decode_sents/sec: 20485.52
2022-04-01 20:25:31,237 epoch 10 - iter 3390/3394 - loss 1.91885056 - samples/sec: 15.99 - decode_sents/sec: 24911.64
2022-04-01 20:25:31,794 ----------------------------------------------------------------------------------------------------
2022-04-01 20:25:31,794 EPOCH 10 done: loss 0.9613 - lr 0.027500000000000004
2022-04-01 20:25:31,794 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:26:04,370 Macro Average: 69.68	Macro avg loss: 4.22
ColumnCorpus-WNUTDOCFULL	69.68	
2022-04-01 20:26:04,429 ----------------------------------------------------------------------------------------------------
2022-04-01 20:26:04,429 BAD EPOCHS (no improvement): 11
2022-04-01 20:26:04,429 GLOBAL BAD EPOCHS (no improvement): 4
2022-04-01 20:26:04,429 ----------------------------------------------------------------------------------------------------
2022-04-01 20:26:04,432 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:26:04,591 epoch 11 - iter 0/3394 - loss 0.62744141 - samples/sec: 12.61 - decode_sents/sec: 103.26
2022-04-01 20:26:55,832 epoch 11 - iter 339/3394 - loss 1.86252293 - samples/sec: 15.52 - decode_sents/sec: 92072.08
2022-04-01 20:27:47,844 epoch 11 - iter 678/3394 - loss 1.66640831 - samples/sec: 15.33 - decode_sents/sec: 109399.79
2022-04-01 20:28:43,221 epoch 11 - iter 1017/3394 - loss 1.66706603 - samples/sec: 14.33 - decode_sents/sec: 89727.64
2022-04-01 20:29:35,479 epoch 11 - iter 1356/3394 - loss 1.76837849 - samples/sec: 15.09 - decode_sents/sec: 89058.85
2022-04-01 20:30:31,269 epoch 11 - iter 1695/3394 - loss 1.77953394 - samples/sec: 14.25 - decode_sents/sec: 30712.57
2022-04-01 20:31:29,326 epoch 11 - iter 2034/3394 - loss 1.78440131 - samples/sec: 13.58 - decode_sents/sec: 19754.77
2022-04-01 20:32:26,305 epoch 11 - iter 2373/3394 - loss 1.80825960 - samples/sec: 13.93 - decode_sents/sec: 29635.86
2022-04-01 20:33:23,180 epoch 11 - iter 2712/3394 - loss 1.78886127 - samples/sec: 13.82 - decode_sents/sec: 47998.82
2022-04-01 20:34:17,915 epoch 11 - iter 3051/3394 - loss 1.78238175 - samples/sec: 14.50 - decode_sents/sec: 154862.39
2022-04-01 20:35:17,157 epoch 11 - iter 3390/3394 - loss 1.80871755 - samples/sec: 13.36 - decode_sents/sec: 82319.82
2022-04-01 20:35:17,850 ----------------------------------------------------------------------------------------------------
2022-04-01 20:35:17,850 EPOCH 11 done: loss 0.9037 - lr 0.025
2022-04-01 20:35:17,850 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:35:54,161 Macro Average: 70.76	Macro avg loss: 4.30
ColumnCorpus-WNUTDOCFULL	70.76	
2022-04-01 20:35:54,222 ----------------------------------------------------------------------------------------------------
2022-04-01 20:35:54,222 BAD EPOCHS (no improvement): 11
2022-04-01 20:35:54,222 GLOBAL BAD EPOCHS (no improvement): 5
2022-04-01 20:35:54,222 ----------------------------------------------------------------------------------------------------
2022-04-01 20:35:54,225 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:35:54,305 epoch 12 - iter 0/3394 - loss 0.11119080 - samples/sec: 25.08 - decode_sents/sec: 218.20
2022-04-01 20:36:46,969 epoch 12 - iter 339/3394 - loss 1.37970021 - samples/sec: 14.99 - decode_sents/sec: 24260.25
2022-04-01 20:37:38,016 epoch 12 - iter 678/3394 - loss 1.65581078 - samples/sec: 15.56 - decode_sents/sec: 40734.23
2022-04-01 20:38:25,201 epoch 12 - iter 1017/3394 - loss 1.64077329 - samples/sec: 16.85 - decode_sents/sec: 22168.04
2022-04-01 20:39:13,419 epoch 12 - iter 1356/3394 - loss 1.68063600 - samples/sec: 16.45 - decode_sents/sec: 22420.06
2022-04-01 20:40:06,318 epoch 12 - iter 1695/3394 - loss 1.76779836 - samples/sec: 14.98 - decode_sents/sec: 93955.07
2022-04-01 20:40:54,115 epoch 12 - iter 2034/3394 - loss 1.78747430 - samples/sec: 16.57 - decode_sents/sec: 77983.28
2022-04-01 20:41:44,421 epoch 12 - iter 2373/3394 - loss 1.78143767 - samples/sec: 15.68 - decode_sents/sec: 23661.73
2022-04-01 20:42:32,720 epoch 12 - iter 2712/3394 - loss 1.76212641 - samples/sec: 16.42 - decode_sents/sec: 18694.04
2022-04-01 20:43:30,436 epoch 12 - iter 3051/3394 - loss 1.78560951 - samples/sec: 13.73 - decode_sents/sec: 20629.97
2022-04-01 20:44:19,474 epoch 12 - iter 3390/3394 - loss 1.81715963 - samples/sec: 16.11 - decode_sents/sec: 139187.42
2022-04-01 20:44:19,781 ----------------------------------------------------------------------------------------------------
2022-04-01 20:44:19,781 EPOCH 12 done: loss 0.9083 - lr 0.022500000000000003
2022-04-01 20:44:19,781 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:44:50,486 Macro Average: 70.23	Macro avg loss: 5.08
ColumnCorpus-WNUTDOCFULL	70.23	
2022-04-01 20:44:50,554 ----------------------------------------------------------------------------------------------------
2022-04-01 20:44:50,554 BAD EPOCHS (no improvement): 11
2022-04-01 20:44:50,554 GLOBAL BAD EPOCHS (no improvement): 6
2022-04-01 20:44:50,554 ----------------------------------------------------------------------------------------------------
2022-04-01 20:44:50,558 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:44:50,797 epoch 13 - iter 0/3394 - loss 0.72528076 - samples/sec: 8.36 - decode_sents/sec: 59.89
2022-04-01 20:45:45,189 epoch 13 - iter 339/3394 - loss 1.68093471 - samples/sec: 14.48 - decode_sents/sec: 20950.04
2022-04-01 20:46:34,508 epoch 13 - iter 678/3394 - loss 1.59610720 - samples/sec: 16.00 - decode_sents/sec: 18127.88
2022-04-01 20:47:33,852 epoch 13 - iter 1017/3394 - loss 1.56129982 - samples/sec: 13.36 - decode_sents/sec: 193952.95
2022-04-01 20:48:32,517 epoch 13 - iter 1356/3394 - loss 1.63400018 - samples/sec: 13.39 - decode_sents/sec: 141549.93
2022-04-01 20:49:30,511 epoch 13 - iter 1695/3394 - loss 1.77531557 - samples/sec: 13.60 - decode_sents/sec: 22773.04
2022-04-01 20:50:19,543 epoch 13 - iter 2034/3394 - loss 1.71204140 - samples/sec: 16.26 - decode_sents/sec: 214962.44
2022-04-01 20:51:12,216 epoch 13 - iter 2373/3394 - loss 1.74054322 - samples/sec: 14.97 - decode_sents/sec: 242040.86
2022-04-01 20:52:00,394 epoch 13 - iter 2712/3394 - loss 1.72516187 - samples/sec: 16.47 - decode_sents/sec: 150669.60
2022-04-01 20:52:55,365 epoch 13 - iter 3051/3394 - loss 1.72871658 - samples/sec: 14.55 - decode_sents/sec: 75197.35
2022-04-01 20:53:53,213 epoch 13 - iter 3390/3394 - loss 1.71473246 - samples/sec: 13.69 - decode_sents/sec: 44332.97
2022-04-01 20:53:53,763 ----------------------------------------------------------------------------------------------------
2022-04-01 20:53:53,764 EPOCH 13 done: loss 0.8567 - lr 0.020000000000000004
2022-04-01 20:53:53,764 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 20:54:26,465 Macro Average: 69.75	Macro avg loss: 5.16
ColumnCorpus-WNUTDOCFULL	69.75	
2022-04-01 20:54:26,546 ----------------------------------------------------------------------------------------------------
2022-04-01 20:54:26,546 BAD EPOCHS (no improvement): 11
2022-04-01 20:54:26,546 GLOBAL BAD EPOCHS (no improvement): 7
2022-04-01 20:54:26,546 ----------------------------------------------------------------------------------------------------
2022-04-01 20:54:26,550 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 20:54:26,881 epoch 14 - iter 0/3394 - loss 4.31152344 - samples/sec: 6.04 - decode_sents/sec: 48.33
2022-04-01 20:55:16,762 epoch 14 - iter 339/3394 - loss 1.91463808 - samples/sec: 15.94 - decode_sents/sec: 68517.21
2022-04-01 20:56:08,818 epoch 14 - iter 678/3394 - loss 1.71496797 - samples/sec: 15.16 - decode_sents/sec: 18302.30
2022-04-01 20:56:57,185 epoch 14 - iter 1017/3394 - loss 1.72485025 - samples/sec: 16.37 - decode_sents/sec: 95315.51
2022-04-01 20:57:47,719 epoch 14 - iter 1356/3394 - loss 1.69641867 - samples/sec: 15.65 - decode_sents/sec: 45130.10
2022-04-01 20:58:35,399 epoch 14 - iter 1695/3394 - loss 1.72499368 - samples/sec: 16.65 - decode_sents/sec: 100902.60
2022-04-01 20:59:23,648 epoch 14 - iter 2034/3394 - loss 1.71359315 - samples/sec: 16.42 - decode_sents/sec: 20832.79
2022-04-01 21:00:11,134 epoch 14 - iter 2373/3394 - loss 1.69907426 - samples/sec: 16.74 - decode_sents/sec: 95826.19
2022-04-01 21:01:04,588 epoch 14 - iter 2712/3394 - loss 1.69473845 - samples/sec: 14.77 - decode_sents/sec: 28162.79
2022-04-01 21:02:01,623 epoch 14 - iter 3051/3394 - loss 1.69287507 - samples/sec: 13.86 - decode_sents/sec: 57676.47
2022-04-01 21:02:55,342 epoch 14 - iter 3390/3394 - loss 1.68524846 - samples/sec: 14.81 - decode_sents/sec: 18428.38
2022-04-01 21:02:55,801 ----------------------------------------------------------------------------------------------------
2022-04-01 21:02:55,802 EPOCH 14 done: loss 0.8435 - lr 0.017499999999999998
2022-04-01 21:02:55,802 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:03:28,510 Macro Average: 70.33	Macro avg loss: 4.97
ColumnCorpus-WNUTDOCFULL	70.33	
2022-04-01 21:03:28,565 ----------------------------------------------------------------------------------------------------
2022-04-01 21:03:28,565 BAD EPOCHS (no improvement): 11
2022-04-01 21:03:28,565 GLOBAL BAD EPOCHS (no improvement): 8
2022-04-01 21:03:28,565 ----------------------------------------------------------------------------------------------------
2022-04-01 21:03:28,568 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:03:28,642 epoch 15 - iter 0/3394 - loss 0.09516907 - samples/sec: 27.16 - decode_sents/sec: 215.03
2022-04-01 21:04:15,528 epoch 15 - iter 339/3394 - loss 1.60000236 - samples/sec: 16.97 - decode_sents/sec: 147680.62
2022-04-01 21:05:05,887 epoch 15 - iter 678/3394 - loss 1.72801211 - samples/sec: 15.61 - decode_sents/sec: 20147.64
2022-04-01 21:05:58,502 epoch 15 - iter 1017/3394 - loss 1.68648466 - samples/sec: 15.03 - decode_sents/sec: 97951.85
2022-04-01 21:06:50,697 epoch 15 - iter 1356/3394 - loss 1.68182214 - samples/sec: 15.18 - decode_sents/sec: 18904.82
2022-04-01 21:07:45,880 epoch 15 - iter 1695/3394 - loss 1.67585680 - samples/sec: 14.33 - decode_sents/sec: 188015.74
2022-04-01 21:08:33,615 epoch 15 - iter 2034/3394 - loss 1.65387485 - samples/sec: 16.65 - decode_sents/sec: 146312.93
2022-04-01 21:09:26,912 epoch 15 - iter 2373/3394 - loss 1.62754863 - samples/sec: 14.90 - decode_sents/sec: 151730.77
2022-04-01 21:10:15,209 epoch 15 - iter 2712/3394 - loss 1.60398614 - samples/sec: 16.40 - decode_sents/sec: 19736.53
2022-04-01 21:11:12,138 epoch 15 - iter 3051/3394 - loss 1.62225172 - samples/sec: 13.87 - decode_sents/sec: 20412.29
2022-04-01 21:12:07,595 epoch 15 - iter 3390/3394 - loss 1.62693026 - samples/sec: 14.37 - decode_sents/sec: 16522.79
2022-04-01 21:12:08,330 ----------------------------------------------------------------------------------------------------
2022-04-01 21:12:08,330 EPOCH 15 done: loss 0.8145 - lr 0.015
2022-04-01 21:12:08,330 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:12:41,235 Macro Average: 70.95	Macro avg loss: 5.19
ColumnCorpus-WNUTDOCFULL	70.95	
2022-04-01 21:12:41,309 ----------------------------------------------------------------------------------------------------
2022-04-01 21:12:41,309 BAD EPOCHS (no improvement): 11
2022-04-01 21:12:41,309 GLOBAL BAD EPOCHS (no improvement): 9
2022-04-01 21:12:41,309 ----------------------------------------------------------------------------------------------------
2022-04-01 21:12:41,312 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:12:41,405 epoch 16 - iter 0/3394 - loss 0.11407471 - samples/sec: 21.54 - decode_sents/sec: 162.63
2022-04-01 21:13:30,295 epoch 16 - iter 339/3394 - loss 1.38865005 - samples/sec: 16.22 - decode_sents/sec: 31384.03
2022-04-01 21:14:22,795 epoch 16 - iter 678/3394 - loss 1.46341726 - samples/sec: 15.12 - decode_sents/sec: 75450.73
2022-04-01 21:15:14,139 epoch 16 - iter 1017/3394 - loss 1.52197702 - samples/sec: 15.47 - decode_sents/sec: 66969.79
2022-04-01 21:16:04,402 epoch 16 - iter 1356/3394 - loss 1.51830717 - samples/sec: 15.82 - decode_sents/sec: 29596.99
2022-04-01 21:17:02,790 epoch 16 - iter 1695/3394 - loss 1.56959974 - samples/sec: 13.56 - decode_sents/sec: 102817.92
2022-04-01 21:17:58,481 epoch 16 - iter 2034/3394 - loss 1.56251922 - samples/sec: 14.27 - decode_sents/sec: 18608.42
2022-04-01 21:18:52,211 epoch 16 - iter 2373/3394 - loss 1.54806617 - samples/sec: 14.78 - decode_sents/sec: 106678.85
2022-04-01 21:19:47,317 epoch 16 - iter 2712/3394 - loss 1.54760603 - samples/sec: 14.32 - decode_sents/sec: 119817.06
2022-04-01 21:20:40,839 epoch 16 - iter 3051/3394 - loss 1.55339951 - samples/sec: 14.73 - decode_sents/sec: 171858.23
2022-04-01 21:21:27,186 epoch 16 - iter 3390/3394 - loss 1.54713836 - samples/sec: 17.20 - decode_sents/sec: 69381.47
2022-04-01 21:21:27,525 ----------------------------------------------------------------------------------------------------
2022-04-01 21:21:27,525 EPOCH 16 done: loss 0.7729 - lr 0.0125
2022-04-01 21:21:27,525 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:21:58,219 Macro Average: 71.01	Macro avg loss: 5.20
ColumnCorpus-WNUTDOCFULL	71.01	
2022-04-01 21:21:58,300 ----------------------------------------------------------------------------------------------------
2022-04-01 21:21:58,301 BAD EPOCHS (no improvement): 11
2022-04-01 21:21:58,301 GLOBAL BAD EPOCHS (no improvement): 10
2022-04-01 21:21:58,301 ----------------------------------------------------------------------------------------------------
2022-04-01 21:21:58,304 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:21:58,411 epoch 17 - iter 0/3394 - loss 3.39944458 - samples/sec: 18.67 - decode_sents/sec: 147.47
2022-04-01 21:22:50,127 epoch 17 - iter 339/3394 - loss 1.45160464 - samples/sec: 15.30 - decode_sents/sec: 28148.86
2022-04-01 21:23:39,010 epoch 17 - iter 678/3394 - loss 1.59644621 - samples/sec: 16.15 - decode_sents/sec: 15237.88
2022-04-01 21:24:25,306 epoch 17 - iter 1017/3394 - loss 1.59563729 - samples/sec: 17.24 - decode_sents/sec: 19067.57
2022-04-01 21:25:23,205 epoch 17 - iter 1356/3394 - loss 1.57659990 - samples/sec: 13.57 - decode_sents/sec: 16881.59
2022-04-01 21:26:13,984 epoch 17 - iter 1695/3394 - loss 1.53868771 - samples/sec: 15.73 - decode_sents/sec: 37968.65
2022-04-01 21:27:04,235 epoch 17 - iter 2034/3394 - loss 1.50908196 - samples/sec: 15.77 - decode_sents/sec: 21999.29
2022-04-01 21:27:54,726 epoch 17 - iter 2373/3394 - loss 1.53455388 - samples/sec: 15.58 - decode_sents/sec: 27058.48
2022-04-01 21:28:43,723 epoch 17 - iter 2712/3394 - loss 1.55011138 - samples/sec: 16.12 - decode_sents/sec: 34209.15
2022-04-01 21:29:37,291 epoch 17 - iter 3051/3394 - loss 1.52956403 - samples/sec: 15.03 - decode_sents/sec: 182771.26
2022-04-01 21:30:29,858 epoch 17 - iter 3390/3394 - loss 1.55174303 - samples/sec: 15.00 - decode_sents/sec: 28178.98
2022-04-01 21:30:30,397 ----------------------------------------------------------------------------------------------------
2022-04-01 21:30:30,397 EPOCH 17 done: loss 0.7766 - lr 0.010000000000000002
2022-04-01 21:30:30,397 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:31:01,246 Macro Average: 70.17	Macro avg loss: 5.47
ColumnCorpus-WNUTDOCFULL	70.17	
2022-04-01 21:31:01,311 ----------------------------------------------------------------------------------------------------
2022-04-01 21:31:01,311 BAD EPOCHS (no improvement): 11
2022-04-01 21:31:01,311 GLOBAL BAD EPOCHS (no improvement): 11
2022-04-01 21:31:01,312 ----------------------------------------------------------------------------------------------------
2022-04-01 21:31:01,315 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:31:01,379 epoch 18 - iter 0/3394 - loss 0.02950287 - samples/sec: 31.20 - decode_sents/sec: 258.66
2022-04-01 21:31:50,503 epoch 18 - iter 339/3394 - loss 1.67842624 - samples/sec: 16.08 - decode_sents/sec: 17724.29
2022-04-01 21:32:35,931 epoch 18 - iter 678/3394 - loss 1.57764753 - samples/sec: 17.66 - decode_sents/sec: 16775.43
2022-04-01 21:33:27,478 epoch 18 - iter 1017/3394 - loss 1.56898091 - samples/sec: 15.32 - decode_sents/sec: 23770.71
2022-04-01 21:34:20,266 epoch 18 - iter 1356/3394 - loss 1.57519955 - samples/sec: 14.87 - decode_sents/sec: 107400.03
2022-04-01 21:35:08,925 epoch 18 - iter 1695/3394 - loss 1.53868563 - samples/sec: 16.35 - decode_sents/sec: 68822.32
2022-04-01 21:36:03,715 epoch 18 - iter 2034/3394 - loss 1.52670562 - samples/sec: 14.57 - decode_sents/sec: 17100.36
2022-04-01 21:37:00,550 epoch 18 - iter 2373/3394 - loss 1.53798436 - samples/sec: 13.98 - decode_sents/sec: 95219.76
2022-04-01 21:37:59,415 epoch 18 - iter 2712/3394 - loss 1.55915614 - samples/sec: 13.47 - decode_sents/sec: 96735.66
2022-04-01 21:38:53,813 epoch 18 - iter 3051/3394 - loss 1.57142490 - samples/sec: 14.61 - decode_sents/sec: 20756.61
2022-04-01 21:39:51,310 epoch 18 - iter 3390/3394 - loss 1.56767936 - samples/sec: 13.80 - decode_sents/sec: 26745.97
2022-04-01 21:39:51,783 ----------------------------------------------------------------------------------------------------
2022-04-01 21:39:51,783 EPOCH 18 done: loss 0.7837 - lr 0.0075
2022-04-01 21:39:51,783 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:40:22,478 Macro Average: 70.66	Macro avg loss: 5.32
ColumnCorpus-WNUTDOCFULL	70.66	
2022-04-01 21:40:22,544 ----------------------------------------------------------------------------------------------------
2022-04-01 21:40:22,544 BAD EPOCHS (no improvement): 11
2022-04-01 21:40:22,544 GLOBAL BAD EPOCHS (no improvement): 12
2022-04-01 21:40:22,544 ----------------------------------------------------------------------------------------------------
2022-04-01 21:40:22,547 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:40:22,613 epoch 19 - iter 0/3394 - loss 0.05535126 - samples/sec: 30.58 - decode_sents/sec: 234.46
2022-04-01 21:41:13,355 epoch 19 - iter 339/3394 - loss 1.58890794 - samples/sec: 15.52 - decode_sents/sec: 44626.56
2022-04-01 21:42:01,660 epoch 19 - iter 678/3394 - loss 1.54943929 - samples/sec: 16.61 - decode_sents/sec: 102145.77
2022-04-01 21:42:54,718 epoch 19 - iter 1017/3394 - loss 1.60361758 - samples/sec: 14.93 - decode_sents/sec: 191304.28
2022-04-01 21:43:46,941 epoch 19 - iter 1356/3394 - loss 1.63258671 - samples/sec: 15.08 - decode_sents/sec: 39127.37
2022-04-01 21:44:40,657 epoch 19 - iter 1695/3394 - loss 1.55675347 - samples/sec: 14.70 - decode_sents/sec: 30974.84
2022-04-01 21:45:28,388 epoch 19 - iter 2034/3394 - loss 1.51059301 - samples/sec: 16.82 - decode_sents/sec: 114491.43
2022-04-01 21:46:26,299 epoch 19 - iter 2373/3394 - loss 1.49726641 - samples/sec: 13.70 - decode_sents/sec: 26214.40
2022-04-01 21:47:20,630 epoch 19 - iter 2712/3394 - loss 1.50678704 - samples/sec: 14.68 - decode_sents/sec: 67991.35
2022-04-01 21:48:18,197 epoch 19 - iter 3051/3394 - loss 1.50999910 - samples/sec: 13.86 - decode_sents/sec: 67191.32
2022-04-01 21:49:13,477 epoch 19 - iter 3390/3394 - loss 1.50668536 - samples/sec: 14.25 - decode_sents/sec: 18970.77
2022-04-01 21:49:13,992 ----------------------------------------------------------------------------------------------------
2022-04-01 21:49:13,992 EPOCH 19 done: loss 0.7537 - lr 0.005000000000000001
2022-04-01 21:49:13,992 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:49:46,784 Macro Average: 70.13	Macro avg loss: 5.28
ColumnCorpus-WNUTDOCFULL	70.13	
2022-04-01 21:49:46,871 ----------------------------------------------------------------------------------------------------
2022-04-01 21:49:46,872 BAD EPOCHS (no improvement): 11
2022-04-01 21:49:46,872 GLOBAL BAD EPOCHS (no improvement): 13
2022-04-01 21:49:46,872 ----------------------------------------------------------------------------------------------------
2022-04-01 21:49:46,875 Current loss interpolation: 1
['xlm-roberta-large']
2022-04-01 21:49:47,128 epoch 20 - iter 0/3394 - loss 0.48193359 - samples/sec: 7.90 - decode_sents/sec: 51.47
2022-04-01 21:50:35,804 epoch 20 - iter 339/3394 - loss 1.71470668 - samples/sec: 16.36 - decode_sents/sec: 99483.58
2022-04-01 21:51:24,511 epoch 20 - iter 678/3394 - loss 1.64223434 - samples/sec: 16.23 - decode_sents/sec: 100684.68
2022-04-01 21:52:11,461 epoch 20 - iter 1017/3394 - loss 1.54497821 - samples/sec: 16.95 - decode_sents/sec: 184263.47
2022-04-01 21:53:08,398 epoch 20 - iter 1356/3394 - loss 1.49324472 - samples/sec: 14.04 - decode_sents/sec: 109340.90
2022-04-01 21:53:58,993 epoch 20 - iter 1695/3394 - loss 1.48184621 - samples/sec: 15.73 - decode_sents/sec: 85897.97
2022-04-01 21:54:51,082 epoch 20 - iter 2034/3394 - loss 1.51644351 - samples/sec: 15.21 - decode_sents/sec: 123581.68
2022-04-01 21:55:48,498 epoch 20 - iter 2373/3394 - loss 1.49224210 - samples/sec: 13.68 - decode_sents/sec: 18553.66
2022-04-01 21:56:43,777 epoch 20 - iter 2712/3394 - loss 1.47031206 - samples/sec: 14.36 - decode_sents/sec: 156035.01
2022-04-01 21:57:30,865 epoch 20 - iter 3051/3394 - loss 1.45198604 - samples/sec: 16.92 - decode_sents/sec: 69557.96
2022-04-01 21:58:26,020 epoch 20 - iter 3390/3394 - loss 1.46783298 - samples/sec: 14.32 - decode_sents/sec: 15704.93
2022-04-01 21:58:26,731 ----------------------------------------------------------------------------------------------------
2022-04-01 21:58:26,731 EPOCH 20 done: loss 0.7338 - lr 0.0025000000000000005
2022-04-01 21:58:26,731 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 None
2022-04-01 21:58:59,671 Macro Average: 70.61	Macro avg loss: 5.36
ColumnCorpus-WNUTDOCFULL	70.61	
2022-04-01 21:58:59,739 ----------------------------------------------------------------------------------------------------
2022-04-01 21:58:59,740 BAD EPOCHS (no improvement): 11
2022-04-01 21:58:59,740 GLOBAL BAD EPOCHS (no improvement): 14
2022-04-01 21:58:59,740 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2107 train
2022-04-01 21:58:59,741 loading file resources/taggers/xlmr-first_10epoch_2batch_2accumulate_0.000005lr_10000lrrate_eng_monolingual_crf_fast_norelearn_sentbatch_sentloss_finetune_nodev_wnut_doc_full_bertscore_eos_ner9/best-model.pt
[2022-04-01 21:59:03,442 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-config.json from cache at /home/miao/.cache/torch/transformers/5ac6d3984e5ca7c5227e4821c65d341900125db538c5f09a1ead14f380def4a7.aa59609b4f56f82fa7699f0d47997566ccc4cf07e484f3a7bc883bd7c5a34488
[2022-04-01 21:59:03,443 INFO] Model config XLMRobertaConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

[2022-04-01 21:59:04,473 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-large-sentencepiece.bpe.model from cache at /home/miao/.cache/torch/transformers/f7e58cf8eef122765ff522a4c7c0805d2fe8871ec58dcb13d0c2764ea3e4a0f3.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed
2022-04-01 21:59:04,984 Testing using best model ...
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2138 train <torch.utils.data.dataset.ConcatDataset object at 0x7f28a2ed5b70>
2022-04-01 21:59:05,203 xlm-roberta-large 559890432
2022-04-01 21:59:05,203 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-01 21:59:37,037 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/xlmr-first_10epoch_2batch_2accumulate_0.000005lr_10000lrrate_eng_monolingual_crf_fast_norelearn_sentbatch_sentloss_finetune_nodev_wnut_doc_full_bertscore_eos_ner9/test.tsv
2022-04-01 21:59:45,362 0.7075	0.5403	0.6127
2022-04-01 21:59:45,363 
MICRO_AVG: acc 0.4417 - f1-score 0.6127
MACRO_AVG: acc 0.373 - f1-score 0.5257666666666667
corporation tp: 32 - fp: 40 - fn: 34 - tn: 32 - precision: 0.4444 - recall: 0.4848 - accuracy: 0.3019 - f1-score: 0.4637
creative-work tp: 66 - fp: 31 - fn: 76 - tn: 66 - precision: 0.6804 - recall: 0.4648 - accuracy: 0.3815 - f1-score: 0.5523
group      tp: 49 - fp: 28 - fn: 116 - tn: 49 - precision: 0.6364 - recall: 0.2970 - accuracy: 0.2539 - f1-score: 0.4050
location   tp: 94 - fp: 30 - fn: 56 - tn: 94 - precision: 0.7581 - recall: 0.6267 - accuracy: 0.5222 - f1-score: 0.6862
person     tp: 317 - fp: 92 - fn: 112 - tn: 317 - precision: 0.7751 - recall: 0.7389 - accuracy: 0.6084 - f1-score: 0.7566
product    tp: 25 - fp: 20 - fn: 102 - tn: 25 - precision: 0.5556 - recall: 0.1969 - accuracy: 0.1701 - f1-score: 0.2908
2022-04-01 21:59:45,363 ----------------------------------------------------------------------------------------------------
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/finetune_trainer.py 2226 train
2022-04-01 21:59:45,363 ----------------------------------------------------------------------------------------------------
2022-04-01 21:59:45,363 current corpus: ColumnCorpus-WNUTDOCFULL
2022-04-01 21:59:45,429 xlm-roberta-large 559890432
2022-04-01 21:59:45,429 first
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/trainers/distillation_trainer.py 1200 final_test
2022-04-01 21:59:47,242 Finished Embeddings Assignments
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2623
[6;30;42mOutPut: [0m /home/miao/6207/lcx/CLNER/flair/models/sequence_tagger_model.py 2633 resources/taggers/xlmr-first_10epoch_2batch_2accumulate_0.000005lr_10000lrrate_eng_monolingual_crf_fast_norelearn_sentbatch_sentloss_finetune_nodev_wnut_doc_full_bertscore_eos_ner9/ColumnCorpus-WNUTDOCFULL-test.tsv
2022-04-01 21:59:55,653 0.7075	0.5403	0.6127
2022-04-01 21:59:55,653 
MICRO_AVG: acc 0.4417 - f1-score 0.6127
MACRO_AVG: acc 0.373 - f1-score 0.5257666666666667
corporation tp: 32 - fp: 40 - fn: 34 - tn: 32 - precision: 0.4444 - recall: 0.4848 - accuracy: 0.3019 - f1-score: 0.4637
creative-work tp: 66 - fp: 31 - fn: 76 - tn: 66 - precision: 0.6804 - recall: 0.4648 - accuracy: 0.3815 - f1-score: 0.5523
group      tp: 49 - fp: 28 - fn: 116 - tn: 49 - precision: 0.6364 - recall: 0.2970 - accuracy: 0.2539 - f1-score: 0.4050
location   tp: 94 - fp: 30 - fn: 56 - tn: 94 - precision: 0.7581 - recall: 0.6267 - accuracy: 0.5222 - f1-score: 0.6862
person     tp: 317 - fp: 92 - fn: 112 - tn: 317 - precision: 0.7751 - recall: 0.7389 - accuracy: 0.6084 - f1-score: 0.7566
product    tp: 25 - fp: 20 - fn: 102 - tn: 25 - precision: 0.5556 - recall: 0.1969 - accuracy: 0.1701 - f1-score: 0.2908

